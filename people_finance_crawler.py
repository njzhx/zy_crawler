import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# ==========================================
# è¾…åŠ©å‡½æ•°
# ==========================================

def get_article_content(url):
    """æŠ“å–æ–‡ç« è¯¦æƒ…é¡µå†…å®¹
    
    Args:
        url: æ–‡ç« è¯¦æƒ…é¡µé“¾æ¥
        
    Returns:
        str: æ–‡ç« å†…å®¹
    """
    if not url:
        return ""
    
    try:
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=20)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« å†…å®¹ï¼ˆæ ¹æ®äººæ°‘ç½‘è¯¦æƒ…é¡µç»“æ„è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        # å¸¸è§çš„å†…å®¹å®¹å™¨é€‰æ‹©å™¨
        content_selectors = [
            '.article-content',       # äººæ°‘ç½‘å¸¸è§çš„å†…å®¹å®¹å™¨
            '.content',                # é€šç”¨å†…å®¹å®¹å™¨
            '.article-body',           # å¦ä¸€ç§å¸¸è§ç»“æ„
            '.main-content',           # ä¸»å†…å®¹åŒº
            '.text_con',               # äººæ°‘ç½‘ç‰¹å®šç»“æ„
            'div[id*="content"]'      # åŒ…å«contentçš„id
        ]
        
        content = ""
        
        # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # æå–æ–‡æœ¬ï¼Œå»é™¤å¤šä½™ç©ºç™½
                content = ' '.join(content_elem.stripped_strings)
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰pæ ‡ç­¾
        if not content:
            p_elems = soup.find_all('p')
            if p_elems:
                content = ' '.join([p.get_text(strip=True) for p in p_elems[:10]])  # å–å‰10ä¸ªpæ ‡ç­¾
        
        # è¿‡æ»¤å¼€å¤´çš„"ç‚¹å‡»æ’­æŠ¥æœ¬æ–‡ï¼Œçº¦  "
        if content.startswith("ç‚¹å‡»æ’­æŠ¥æœ¬æ–‡ï¼Œçº¦  "):
            # æ‰¾åˆ°"çº¦  "åé¢çš„å†…å®¹
            prefix_part = "ç‚¹å‡»æ’­æŠ¥æœ¬æ–‡ï¼Œçº¦  "
            after_prefix = content[len(prefix_part):]
            
            # å°è¯•æ‰¾åˆ°"å­—"å­—ç¬¦ï¼Œè¿™åº”è¯¥æ˜¯å­—æ•°æè¿°çš„ç»“æŸ
            char_pos = after_prefix.find("å­—")
            if char_pos != -1:
                # ä¿ç•™"å­—"åçš„æ‰€æœ‰å†…å®¹
                content = after_prefix[char_pos+1:].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°"å­—"å­—ç¬¦ï¼Œå°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ªç©ºæ ¼
                space_pos = after_prefix.find(" ")
                if space_pos != -1:
                    # ä¿ç•™ç©ºæ ¼åçš„æ‰€æœ‰å†…å®¹
                    content = after_prefix[space_pos:].strip()
                else:
                    # å¦‚æœæ—¢æ²¡æœ‰æ‰¾åˆ°"å­—"ä¹Ÿæ²¡æœ‰æ‰¾åˆ°ç©ºæ ¼ï¼Œç›´æ¥ä¿ç•™å‰ç¼€åçš„æ‰€æœ‰å†…å®¹
                    content = after_prefix.strip()
        
        # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…å­˜å‚¨è¿‡å¤§çš„æ•°æ®
        if len(content) > 5000:
            content = content[:5000] + "..."
        
        return content
        
    except Exception as e:
        # é™é»˜å¤±è´¥ï¼Œä¸å½±å“ä¸»çˆ¬è™«æ‰§è¡Œ
        print(f"âš ï¸  æŠ“å–è¯¦æƒ…é¡µå¤±è´¥ - {url[:50]}...")
        return ""

# ==========================================
# 1. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–äººæ°‘ç½‘è´¢ç»é¢‘é“æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    """
    policies = []
    url = "http://finance.people.com.cn/GB/70846/index.html"
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        # åˆ›å»º UTC+8 æ—¶åŒº
        tz_utc8 = timezone(timedelta(hours=8))
        # è·å–åŒ—äº¬æ—¶é—´
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"ğŸ“… è¿è¡Œæ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ï¼š{today}")
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        # åŒæ—¶æ˜¾ç¤º UTC æ—¶é—´ï¼Œä¾¿äºè°ƒè¯•
        utc_now = datetime.utcnow()
        print(f"ğŸŒ è¿è¡Œæ—¶é—´ï¼ˆUTCï¼‰ï¼š{utc_now.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨ï¼ˆç›´æ¥æŸ¥æ‰¾æ‰€æœ‰liå…ƒç´ ï¼‰
        policy_items = soup.find_all('li')
        
        filtered_count = 0
        
        for item in policy_items:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = item.find('a')
            if not title_elem:
                continue
            
            # æå–æ–‡æœ¬
            text = item.get_text(strip=True)
            
            # æå–æ—¥æœŸï¼ˆä»æ–‡æœ¬æœ«å°¾æå–ï¼‰
            import re
            date_pattern = re.compile(r'(\d{4}-\d{2}-\d{2})$')
            date_match = date_pattern.search(text)
            
            pub_at = None
            if date_match:
                date_str = date_match.group(1)
                try:
                    pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                except ValueError:
                    pass
            
            # æå–æ ‡é¢˜ï¼ˆå»é™¤æ—¥æœŸéƒ¨åˆ†ï¼‰
            if date_match:
                title = text[:date_match.start()].strip()
            else:
                title = title_elem.get_text(strip=True)
            
            # æå–é“¾æ¥
            policy_url = title_elem.get('href')
            
            # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
            if policy_url and not policy_url.startswith('http'):
                # æ£€æŸ¥æ˜¯å¦æ˜¯ç›¸å¯¹è·¯å¾„
                if policy_url.startswith('/'):
                    policy_url = f"http://finance.people.com.cn{policy_url}"
                else:
                    policy_url = f"http://finance.people.com.cn/GB/70846/{policy_url}"
            
            # è¿‡æ»¤ï¼šåªä¿ç•™å‰ä¸€å¤©çš„æ–‡ç« 
            if pub_at != yesterday:
                filtered_count += 1
                continue
            
            # æå–å†…å®¹ï¼ˆæŠ“å–è¯¦æƒ…é¡µå†…å®¹ï¼‰
            content = get_article_content(policy_url)
            
            # æ„å»ºæ”¿ç­–æ•°æ®
            policy_data = {
                'title': title,
                'url': policy_url,
                'pub_at': pub_at,
                'content': content,
                'selected': False,
                'category': '',  # ç•™ç©ºï¼Œä¸è®¾ç½®é»˜è®¤å€¼
                'source': 'äººæ°‘ç½‘è´¢ç»'
            }
            
            policies.append(policy_data)
        
        print(f"âœ… äººæ°‘ç½‘è´¢ç»çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
    except Exception as e:
        print(f"âŒ äººæ°‘ç½‘è´¢ç»çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
    
    return policies

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    
    ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“å·¥å…·å‡½æ•°
    """
    return save_to_policy(data_list, "äººæ°‘ç½‘è´¢ç»çˆ¬è™«")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œäººæ°‘ç½‘è´¢ç»çˆ¬è™«"""
    try:
        data = scrape_data()
        result = save_to_supabase(data)
        # è¿”å›å®é™…æŠ“å–çš„æ•°æ®ï¼Œçˆ¬è™«ç®¡ç†å™¨ä¼šæ ¹æ®æ­¤è®¡ç®—æ•°é‡
        return result
    except Exception as e:
        print(f"âŒ äººæ°‘ç½‘è´¢ç»çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        return []

if __name__ == "__main__":
    run()
