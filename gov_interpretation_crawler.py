import os
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# çˆ¬è™«é…ç½®
TARGET_URL = "https://www.gov.cn/zhengce/jiedu/"

# ==========================================
# 2. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    """
    policies = []
    url = TARGET_URL + "index.htm"
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"ğŸ“… è¿è¡Œæ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ï¼š{today}")
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾JSONæ•°æ®URL
        json_url = None
        scripts = soup.find_all('script')
        for script in scripts:
            script_content = script.string
            if script_content and 'list-1-ajax-id' in script_content:
                json_match = re.search(r'url:\s*["\']([^"\']+)ZCJD_QZ\.json["\']', script_content)
                if json_match:
                    json_path = json_match.group(1) + "ZCJD_QZ.json"
                    if json_path.startswith('./'):
                        json_url = f"https://www.gov.cn/zhengce/jiedu/{json_path[2:]}"
                    else:
                        json_url = f"https://www.gov.cn/zhengce/jiedu/{json_path}"
                    break
        
        # è®¿é—®JSONæ•°æ®æ–‡ä»¶
        json_policies = []
        try:
            if not json_url:
                json_url = "https://www.gov.cn/zhengce/jiedu/ZCJD_QZ.json"
            
            response = requests.get(json_url, timeout=15)
            if response.status_code == 200:
                import json
                data = response.json()
                
                if isinstance(data, list):
                    # ç­›é€‰ç›®æ ‡æ—¥æœŸçš„æ–‡ç« 
                    total_count = len(data)
                    filtered_count = 0
                    
                    for article in data:
                        if isinstance(article, dict) and 'TITLE' in article and 'URL' in article and 'DOCRELPUBTIME' in article:
                            try:
                                pub_at = datetime.strptime(article['DOCRELPUBTIME'], '%Y-%m-%d').date()
                                if pub_at == yesterday:
                                    # è·å–æ–‡ç« URL
                                    article_url = article['URL'] if article['URL'].startswith('http') else f"https://www.gov.cn{article['URL']}"
                                    
                                    # æŠ“å–è¯¦æƒ…é¡µå†…å®¹
                                    content = ""
                                    try:
                                        detail_response = requests.get(article_url, timeout=15)
                                        detail_response.raise_for_status()
                                        detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                                        # ä½¿ç”¨ç”¨æˆ·æä¾›çš„XPathå¯¹åº”çš„CSSé€‰æ‹©å™¨
                                        content_elem = detail_soup.select_one('#UCAP-CONTENT')
                                        if content_elem:
                                            content = content_elem.get_text(strip=True)
                                    except Exception:
                                        pass
                                    
                                    policy_data = {
                                        'title': article['TITLE'],
                                        'url': article_url,
                                        'pub_at': pub_at,
                                        'content': content,
                                        'selected': False,
                                        'category': '',
                                        'source': 'ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»'
                                    }
                                    json_policies.append(policy_data)
                                else:
                                    filtered_count += 1
                            except Exception:
                                filtered_count += 1
                                pass
                    
                    if json_policies:
                        print(f"âœ… æˆåŠŸæŠ“å– {len(json_policies)} æ¡ç›®æ ‡æ—¥æœŸçš„æ–‡ç« ")
                        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
                        return json_policies
        except Exception as e:
            print(f"âš ï¸  è®¿é—®JSONæ–‡ä»¶å¤±è´¥ï¼š{e}")
        
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
    
    return policies

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    
    ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“å·¥å…·å‡½æ•°
    """
    return save_to_policy(data_list, "ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«"""
    try:
        data = scrape_data()
        result = save_to_supabase(data)
        return result
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šè¿è¡Œå¤±è´¥ - {e}")
        return []

if __name__ == "__main__":
    run()
