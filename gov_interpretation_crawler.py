import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# ==========================================
# 2. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    """
    policies = []
    url = "https://www.gov.cn/zhengce/jiedu/index.htm"
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        # åˆ›å»º UTC+8 æ—¶åŒº
        tz_utc8 = timezone(timedelta(hours=8))
        # è·å–åŒ—äº¬æ—¶é—´
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"ğŸ“… è¿è¡Œæ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ï¼š{today}")
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        # åŒæ—¶æ˜¾ç¤º UTC æ—¶é—´ï¼Œä¾¿äºè°ƒè¯•
        utc_now = datetime.utcnow()
        print(f"ğŸŒ è¿è¡Œæ—¶é—´ï¼ˆUTCï¼‰ï¼š{utc_now.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ”¿ç­–è§£è¯»åˆ—è¡¨ï¼ˆæ ¹æ®å®é™…ç½‘é¡µç»“æ„è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        # å®é™…é¡µé¢ç»“æ„ï¼š.news_box .list åŒ…å«æ–‡ç« é¡¹
        news_box = soup.select_one('.news_box')
        policy_items = []
        
        print(f"\nğŸ” å¼€å§‹æŸ¥æ‰¾æ–‡ç« é¡¹...")
        
        if news_box:
            print("âœ… æ‰¾åˆ° news_box å®¹å™¨")
            # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨æŸ¥æ‰¾æ–‡ç« é¡¹
            possible_selectors = [
                '.list > li',            # åˆ—è¡¨ä¸­çš„li
                '.list > div',           # åˆ—è¡¨ä¸­çš„div
                'li',                    # æ‰€æœ‰li
                '.item',                 # æ–‡ç« é¡¹
                '.article-item'          # æ–‡ç« é¡¹
            ]
            
            for selector in possible_selectors:
                items = news_box.select(selector)
                if items:
                    policy_items = items
                    print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(items)} ä¸ªæ–‡ç« é¡¹")
                    break
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°news_boxï¼Œå°è¯•ç›´æ¥æŸ¥æ‰¾
            policy_items = soup.select('li')
            print(f"âš ï¸  æœªæ‰¾åˆ° news_boxï¼Œç›´æ¥æŸ¥æ‰¾ li å…ƒç´ ï¼Œæ‰¾åˆ° {len(policy_items)} ä¸ª")
        
        print(f"\nğŸ“‹ æœ€ç»ˆæ‰¾åˆ° {len(policy_items)} ä¸ªæ–‡ç« é¡¹")
        
        filtered_count = 0
        
        for item in policy_items:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = item.select_one('a')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            policy_url = title_elem.get('href')
            
            # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
            if policy_url and not policy_url.startswith('http'):
                policy_url = f"https://www.gov.cn{policy_url}"
            
            # æå–å‘å¸ƒæ—¥æœŸ
            pub_at = None
            
            # å°è¯•ä¸åŒçš„æ—¥æœŸå…ƒç´ é€‰æ‹©å™¨
            date_selectors = [
                '.date',            # classä¸ºdateçš„å…ƒç´ 
                'span.date',        # spanæ ‡ç­¾ä¸”classä¸ºdate
                '.time',            # classä¸ºtimeçš„å…ƒç´ 
                'span.time'         # spanæ ‡ç­¾ä¸”classä¸ºtime
            ]
            
            for selector in date_selectors:
                date_elem = item.select_one(selector)
                if date_elem:
                    date_str = date_elem.get_text(strip=True)
                    try:
                        # æ¸…ç†æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç§»é™¤å¤šä½™å­—ç¬¦ï¼‰
                        import re
                        date_match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
                        if date_match:
                            date_str = date_match.group(0)
                            pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                            break
                    except ValueError:
                        pass
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸå…ƒç´ ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
            if not pub_at:
                text = item.get_text(strip=True)
                import re
                date_match = re.search(r'\d{4}-\d{2}-\d{2}', text)
                if date_match:
                    try:
                        date_str = date_match.group(0)
                        pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                    except ValueError:
                        pass
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºæå–çš„æ—¥æœŸ
            if pub_at:
                print(f"ğŸ“… æå–æ—¥æœŸï¼š{pub_at}ï¼Œç›®æ ‡æ—¥æœŸï¼š{yesterday}")
            else:
                print(f"â“ æœªæå–åˆ°æ—¥æœŸ - æ ‡é¢˜ï¼š{title[:30]}...")
            
            # è¿‡æ»¤ï¼šåªä¿ç•™å‰ä¸€å¤©çš„æ–‡ç« 
            if pub_at != yesterday:
                filtered_count += 1
                if pub_at:
                    print(f"â­ï¸  è¿‡æ»¤æ‰éç›®æ ‡æ—¥æœŸæ–‡ç« ï¼š{pub_at}")
                else:
                    print(f"â­ï¸  è¿‡æ»¤æ‰æ— æ—¥æœŸæ–‡ç« ")
                continue
            
            # è°ƒè¯•ï¼šæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« 
            print(f"âœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæ–‡ç« ï¼š{title[:30]}...")
            
            # æå–å†…å®¹ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦è¿›å…¥è¯¦æƒ…é¡µæŠ“å–ï¼‰
            content = ""  # å¯ä»¥åç»­å®ç°è¯¦æƒ…é¡µæŠ“å–
            
            # æ„å»ºæ”¿ç­–æ•°æ®
            policy_data = {
                'title': title,
                'url': policy_url,
                'pub_at': pub_at,
                'content': content,
                'selected': False,
                'category': '',  # ç•™ç©ºï¼Œä¸è®¾ç½®é»˜è®¤å€¼
                'source': 'ä¸­å›½æ”¿åºœç½‘'
            }
            
            policies.append(policy_data)
        
        print(f"âœ… ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
    
    return policies

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    
    ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“å·¥å…·å‡½æ•°
    """
    return save_to_policy(data_list, "ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«"""
    try:
        data = scrape_data()
        result = save_to_supabase(data)
        return result
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        return []

if __name__ == "__main__":
    run()
