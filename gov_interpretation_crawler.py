import os
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# ==========================================
# 2. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    """
    policies = []
    url = "https://www.gov.cn/zhengce/jiedu/index.htm"
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        # åˆ›å»º UTC+8 æ—¶åŒº
        tz_utc8 = timezone(timedelta(hours=8))
        # è·å–åŒ—äº¬æ—¶é—´
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"ğŸ“… è¿è¡Œæ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ï¼š{today}")
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        # åŒæ—¶æ˜¾ç¤º UTC æ—¶é—´ï¼Œä¾¿äºè°ƒè¯•
        utc_now = datetime.utcnow()
        print(f"ğŸŒ è¿è¡Œæ—¶é—´ï¼ˆUTCï¼‰ï¼š{utc_now.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ”¿ç­–è§£è¯»åˆ—è¡¨ï¼ˆæ ¹æ®å®é™…ç½‘é¡µç»“æ„è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        # é¦–å…ˆå°è¯•ç”¨æˆ·æä¾›çš„xpathå¯¹åº”çš„é€‰æ‹©å™¨
        list_container = soup.select_one('#list-1-ajax-id')
        policy_items = []
        
        print(f"\nğŸ” å¼€å§‹æŸ¥æ‰¾æ–‡ç« é¡¹...")
        
        if list_container:
            print("âœ… æ‰¾åˆ° list-1-ajax-id å®¹å™¨")
            # æ£€æŸ¥å®¹å™¨æ˜¯å¦ä¸ºç©º
            if not list_container.find_all():
                print("âš ï¸  list-1-ajax-id å®¹å™¨ä¸ºç©ºï¼Œå¯èƒ½æ˜¯åŠ¨æ€åŠ è½½çš„")
            else:
                # å°è¯•åœ¨list-1-ajax-idä¸­æŸ¥æ‰¾æ–‡ç« é¡¹
                possible_selectors = [
                    'li',                    # æ‰€æœ‰li
                    'div',                   # æ‰€æœ‰div
                    '.item',                 # æ–‡ç« é¡¹
                    '.article-item',         # æ–‡ç« é¡¹
                    '*'                      # æ‰€æœ‰å­å…ƒç´ 
                ]
                
                for selector in possible_selectors:
                    items = list_container.select(selector)
                    if items:
                        policy_items = items
                        print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨ '{selector}' æ‰¾åˆ° {len(items)} ä¸ªæ–‡ç« é¡¹")
                        break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„å®¹å™¨
        if not policy_items:
            print("ğŸ” å°è¯•æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ–‡ç« å®¹å™¨...")
            
            # å°è¯•å¸¸è§çš„æ–‡ç« åˆ—è¡¨å®¹å™¨
            possible_containers = [
                '.news_box',              # æ–°é—»æ¡†
                '.list_box',              # åˆ—è¡¨æ¡†
                '.article_list',          # æ–‡ç« åˆ—è¡¨
                '.news_list',             # æ–°é—»åˆ—è¡¨
                '.content_list',          # å†…å®¹åˆ—è¡¨
                'ul',                     # æ‰€æœ‰ul
                'ol'                      # æ‰€æœ‰ol
            ]
            
            for container_selector in possible_containers:
                containers = soup.select(container_selector)
                for container in containers:
                    # å°è¯•åœ¨å®¹å™¨ä¸­æŸ¥æ‰¾æ–‡ç« é¡¹
                    possible_item_selectors = [
                        '.item',                 # æ–‡ç« é¡¹
                        '.article-item',         # æ–‡ç« é¡¹
                        'li',                    # æ‰€æœ‰li
                    ]
                    
                    for item_selector in possible_item_selectors:
                        items = container.select(item_selector)
                        if items:
                            # æ£€æŸ¥è¿™äº›é¡¹æ˜¯å¦çœŸçš„åŒ…å«æ–‡ç« æ•°æ®ï¼ˆæ˜¯å¦æœ‰é“¾æ¥ä¸”ä¸æ˜¯å¯¼èˆªé“¾æ¥ï¼‰
                            valid_items = []
                            for item in items:
                                a_tag = item.find('a')
                                if a_tag:
                                    href = a_tag.get('href', '')
                                    text = a_tag.get_text(strip=True)
                                    # è¿‡æ»¤æ‰å¯¼èˆªé“¾æ¥
                                    if href and text and not any(keyword in text for keyword in ['é¦–é¡µ', 'ç®€', 'ç¹', 'EN', 'ç™»å½•', 'ä¸ªäººä¸­å¿ƒ', 'é€€å‡º', 'é‚®ç®±', 'æ— éšœç¢']):
                                        valid_items.append(item)
                            
                            if valid_items:
                                policy_items = valid_items
                                print(f"âœ… åœ¨å®¹å™¨ '{container_selector}' ä¸­ä½¿ç”¨é€‰æ‹©å™¨ '{item_selector}' æ‰¾åˆ° {len(valid_items)} ä¸ªæœ‰æ•ˆæ–‡ç« é¡¹")
                                # æ‰“å°å‰å‡ ä¸ªé¡¹çš„å†…å®¹é¢„è§ˆ
                                for i, valid_item in enumerate(valid_items[:3]):
                                    item_content = valid_item.prettify()[:500]
                                    print(f"ğŸ“ ç¬¬{i+1}ä¸ªæ–‡ç« é¡¹å†…å®¹é¢„è§ˆï¼š{item_content}...")
                                break
                    if policy_items:
                        break
                if policy_items:
                    break
        
        # æ¸…é™¤å½“å‰æ‰¾åˆ°çš„å¯¼èˆªé“¾æ¥ï¼Œé‡æ–°æœç´¢
        policy_items = []
        print("ğŸ” æ¸…é™¤å¯¼èˆªé“¾æ¥ï¼Œé‡æ–°æœç´¢å®é™…çš„æ”¿ç­–è§£è¯»æ–‡ç« ...")
        
        # 1. é¦–å…ˆå°è¯•æŸ¥æ‰¾news_boxå®¹å™¨
        news_box = soup.find('div', class_='news_box')
        if news_box:
            print("âœ… æ‰¾åˆ° news_box å®¹å™¨ï¼Œå¼€å§‹æœç´¢å…¶ä¸­çš„æ–‡ç« ...")
            
            # æŸ¥æ‰¾news_boxä¸­çš„æ‰€æœ‰å­å…ƒç´ 
            all_children = news_box.find_all(['li', 'div', 'p', 'span', 'a'])
            
            # éå†æ‰€æœ‰å…ƒç´ ï¼ŒæŸ¥æ‰¾åŒ…å«é“¾æ¥å’Œæ—¥æœŸçš„ç»„åˆ
            for child in all_children:
                # æŸ¥æ‰¾é“¾æ¥
                link = child.find('a')
                if link:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    # è¿‡æ»¤æ¡ä»¶
                    if (href and text and 
                        not any(keyword in text for keyword in ['é¦–é¡µ', 'ç®€', 'ç¹', 'EN', 'ç™»å½•', 'ä¸ªäººä¸­å¿ƒ', 'é€€å‡º', 'é‚®ç®±', 'æ— éšœç¢', 'å…¨å›½äººå¤§', 'å…¨å›½æ”¿å', 'å›½å®¶ç›‘å¯Ÿå§”å‘˜ä¼š', 'æœ€é«˜äººæ°‘æ³•é™¢', 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢'])):
                        
                        # æŸ¥æ‰¾é™„è¿‘çš„æ—¥æœŸå…ƒç´ 
                        # æ£€æŸ¥å½“å‰å…ƒç´ 
                        current_text = child.get_text()
                        date_match = re.search(r'\d{4}-\d{2}-\d{2}', current_text)
                        
                        # æ£€æŸ¥çˆ¶å…ƒç´ 
                        if not date_match:
                            parent = child.find_parent(['li', 'div'])
                            if parent:
                                parent_text = parent.get_text()
                                date_match = re.search(r'\d{4}-\d{2}-\d{2}', parent_text)
                        
                        # æ£€æŸ¥å…„å¼Ÿå…ƒç´ 
                        if not date_match:
                            siblings = child.find_next_siblings(['span', 'div', 'p'])
                            for sibling in siblings[:3]:
                                sibling_text = sibling.get_text()
                                date_match = re.search(r'\d{4}-\d{2}-\d{2}', sibling_text)
                                if date_match:
                                    break
                        
                        if date_match:
                            # ç¡®å®šåŒ…å«é“¾æ¥å’Œæ—¥æœŸçš„å®¹å™¨
                            container = child.find_parent(['li', 'div']) if child.name != 'li' and child.name != 'div' else child
                            policy_items.append(container)
                            print(f"âœ… æ‰¾åˆ°æ”¿ç­–è§£è¯»æ–‡ç« ï¼š{text[:50]}... æ—¥æœŸï¼š{date_match.group(0)}")
                            # é™åˆ¶æ‰¾åˆ°çš„æ–‡ç« æ•°é‡
                            if len(policy_items) >= 10:
                                break
            
            if policy_items:
                print(f"âœ… åœ¨ news_box ä¸­æ‰¾åˆ° {len(policy_items)} ä¸ªæ”¿ç­–è§£è¯»æ–‡ç« ")
        
        # 2. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ—¥æœŸçš„spanå…ƒç´ 
        if not policy_items:
            print("ğŸ” å°è¯•æŸ¥æ‰¾æ‰€æœ‰åŒ…å«æ—¥æœŸçš„spanå…ƒç´ ...")
            # å¯¼å…¥reæ¨¡å—ï¼ˆç¡®ä¿åœ¨ä½œç”¨åŸŸå†…å¯ç”¨ï¼‰
            import re
            # æŸ¥æ‰¾æ‰€æœ‰spanå…ƒç´ ï¼Œç„¶åé€ä¸ªæ£€æŸ¥
            all_spans = soup.find_all('span')
            date_spans = []
            
            for span in all_spans:
                text = span.get_text(strip=True)
                if text:
                    date_match = re.search(r'\d{4}-\d{2}-\d{2}', text)
                    if date_match:
                        date_spans.append(span)
            
            for span in date_spans:
                # æŸ¥æ‰¾é™„è¿‘çš„é“¾æ¥
                parent = span.find_parent(['li', 'div'])
                if parent:
                    link = parent.find('a')
                    if link:
                        href = link.get('href', '')
                        text = link.get_text(strip=True)
                        if href and text:
                            policy_items.append(parent)
                            date_text = span.get_text(strip=True)
                            print(f"âœ… æ‰¾åˆ°æ”¿ç­–è§£è¯»æ–‡ç« ï¼š{text[:50]}... æ—¥æœŸï¼š{date_text}")
                            # é™åˆ¶æ‰¾åˆ°çš„æ–‡ç« æ•°é‡
                            if len(policy_items) >= 10:
                                break
            
            if policy_items:
                print(f"âœ… é€šè¿‡æ—¥æœŸspanæ‰¾åˆ° {len(policy_items)} ä¸ªæ”¿ç­–è§£è¯»æ–‡ç« ")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥çœ‹scriptæ ‡ç­¾ä¸­çš„æ•°æ®
        if not policy_items:
            print("ğŸ” å°è¯•æŸ¥çœ‹scriptæ ‡ç­¾ä¸­çš„æ•°æ®...")
            scripts = soup.find_all('script')
            
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«å†…è”æ–‡ç« æ•°æ®çš„scriptæ ‡ç­¾
            for i, script in enumerate(scripts):
                script_content = script.string
                if script_content:
                    # æŸ¥æ‰¾åŒ…å«å¤§é‡æ–‡æœ¬å’Œæ—¥æœŸçš„scriptæ ‡ç­¾
                    if len(script_content) > 1000 and re.search(r'\d{4}-\d{2}-\d{2}', script_content):
                        print(f"ğŸ“ å‘ç°å¯èƒ½åŒ…å«å†…è”æ–‡ç« æ•°æ®çš„scriptæ ‡ç­¾ #{i}ï¼Œé•¿åº¦ï¼š{len(script_content)}...")
                        # æ‰“å°å‰2000ä¸ªå­—ç¬¦
                        print(f"ğŸ“ scriptå†…å®¹é¢„è§ˆï¼š{script_content[:2000]}...")
                        break
            
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«æ–‡ç« åˆ—è¡¨çš„scriptæ ‡ç­¾
            for i, script in enumerate(scripts):
                script_content = script.string
                if script_content:
                    # æŸ¥æ‰¾åŒ…å«å¤šä¸ªæ ‡é¢˜å’Œé“¾æ¥çš„scriptæ ‡ç­¾
                    if script_content.count('title') > 3 and script_content.count('href') > 3:
                        print(f"ğŸ“ å‘ç°å¯èƒ½åŒ…å«æ–‡ç« åˆ—è¡¨çš„scriptæ ‡ç­¾ #{i}ï¼Œé•¿åº¦ï¼š{len(script_content)}...")
                        # æ‰“å°å‰2000ä¸ªå­—ç¬¦
                        print(f"ğŸ“ scriptå†…å®¹é¢„è§ˆï¼š{script_content[:2000]}...")
                        break
            
            # æŸ¥æ‰¾å¯èƒ½åŒ…å«AJAXé…ç½®çš„scriptæ ‡ç­¾
            for i, script in enumerate(scripts):
                script_content = script.string
                if script_content:
                    if 'ajax' in script_content and 'url' in script_content and ('zhengce' in script_content or 'jiedu' in script_content):
                        print(f"ğŸ“ å‘ç°å¯èƒ½åŒ…å«AJAXé…ç½®çš„scriptæ ‡ç­¾ #{i}ï¼Œé•¿åº¦ï¼š{len(script_content)}...")
                        # æ‰“å°å‰2000ä¸ªå­—ç¬¦
                        print(f"ğŸ“ scriptå†…å®¹é¢„è§ˆï¼š{script_content[:2000]}...")
                        break
        
        # æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦æœ‰iframe
        if not policy_items:
            print("ğŸ” å°è¯•æ£€æŸ¥é¡µé¢ä¸­çš„iframe...")
            iframes = soup.find_all('iframe')
            for iframe in iframes:
                src = iframe.get('src', '')
                if src:
                    print(f"ğŸ“ å‘ç°iframeï¼š{src}")
                    # å°è¯•è®¿é—®iframeçš„å†…å®¹
                    try:
                        iframe_url = src if src.startswith('http') else f"https://www.gov.cn{src}"
                        print(f"ğŸ“ å°è¯•è®¿é—®iframeå†…å®¹ï¼š{iframe_url}")
                        iframe_response = requests.get(iframe_url, timeout=10)
                        if iframe_response.status_code == 200:
                            print(f"âœ… iframeè¯·æ±‚æˆåŠŸï¼š{iframe_url}")
                            # æ£€æŸ¥iframeå†…å®¹æ˜¯å¦åŒ…å«æ–‡ç« æ•°æ®
                            iframe_soup = BeautifulSoup(iframe_response.content, 'html.parser')
                            iframe_articles = iframe_soup.find_all(['li', 'div'], class_=lambda x: x and 'article' in x)
                            if iframe_articles:
                                print(f"âœ… åœ¨iframeä¸­æ‰¾åˆ° {len(iframe_articles)} ä¸ªæ–‡ç« é¡¹")
                                # æ‰“å°ç¬¬ä¸€ä¸ªæ–‡ç« é¡¹
                                if iframe_articles:
                                    print(f"ğŸ“ iframeæ–‡ç« é¡¹é¢„è§ˆï¼š{iframe_articles[0].prettify()[:500]}...")
                            break
                    except Exception as e:
                        print(f"âš ï¸  è®¿é—®iframeå¤±è´¥ï¼š{e}")
        
        # å°è¯•æ£€æŸ¥é¡µé¢ä¸­æ‰€æœ‰å¯èƒ½çš„æ–‡ç« å®¹å™¨
        if not policy_items:
            print("ğŸ” å°è¯•æ£€æŸ¥é¡µé¢ä¸­æ‰€æœ‰å¯èƒ½çš„æ–‡ç« å®¹å™¨...")
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«æ–‡ç« çš„å®¹å™¨
            possible_containers = soup.find_all(['div', 'section', 'main'], class_=lambda x: x and any(keyword in x for keyword in ['content', 'article', 'list', 'news', 'body', 'main']))
            
            for i, container in enumerate(possible_containers):
                # æ£€æŸ¥å®¹å™¨æ˜¯å¦åŒ…å«é“¾æ¥å’Œæ—¥æœŸ
                links = container.find_all('a')
                if links:
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ—¥æœŸä¿¡æ¯
                    container_text = container.get_text()
                    if re.search(r'\d{4}-\d{2}-\d{2}', container_text):
                        print(f"ğŸ“ å‘ç°å¯èƒ½åŒ…å«æ–‡ç« çš„å®¹å™¨ #{i}ï¼ŒåŒ…å« {len(links)} ä¸ªé“¾æ¥")
                        print(f"ğŸ“ å®¹å™¨å†…å®¹é¢„è§ˆï¼š{container.prettify()[:1000]}...")
                        # æå–å¯èƒ½çš„æ–‡ç« é¡¹
                        for link in links[:5]:
                            text = link.get_text(strip=True)
                            href = link.get('href', '')
                            if text and href:
                                print(f"ğŸ“ é“¾æ¥ï¼š{text[:50]}... {href}")
                        break
        
        # å°è¯•ç›´æ¥åˆ†æé¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥
        if not policy_items:
            print("ğŸ” å°è¯•åˆ†æé¡µé¢ä¸­çš„æ‰€æœ‰é“¾æ¥...")
            all_links = soup.find_all('a')
            print(f"ğŸ“ æ‰¾åˆ° {len(all_links)} ä¸ªé“¾æ¥")
            
            # è¿‡æ»¤å‡ºå¯èƒ½çš„æ”¿ç­–è§£è¯»æ–‡ç« é“¾æ¥
            for link in all_links:
                text = link.get_text(strip=True)
                href = link.get('href', '')
                
                if (text and href and 
                    not any(keyword in text for keyword in ['é¦–é¡µ', 'ç®€', 'ç¹', 'EN', 'ç™»å½•', 'ä¸ªäººä¸­å¿ƒ', 'é€€å‡º', 'é‚®ç®±', 'æ— éšœç¢', 'å…¨å›½äººå¤§', 'å…¨å›½æ”¿å', 'å›½å®¶ç›‘å¯Ÿå§”å‘˜ä¼š', 'æœ€é«˜äººæ°‘æ³•é™¢', 'æœ€é«˜äººæ°‘æ£€å¯Ÿé™¢']) and
                    ('zhengce' in href or 'jiedu' in href)):
                    
                    print(f"ğŸ“ å¯èƒ½çš„æ”¿ç­–è§£è¯»é“¾æ¥ï¼š{text[:50]}... {href}")
                    # æŸ¥æ‰¾é“¾æ¥é™„è¿‘çš„æ—¥æœŸ
                    parent = link.find_parent(['li', 'div', 'p'])
                    if parent:
                        parent_text = parent.get_text()
                        date_match = re.search(r'\d{4}-\d{2}-\d{2}', parent_text)
                        if date_match:
                            print(f"âœ… æ‰¾åˆ°å¸¦æ—¥æœŸçš„æ”¿ç­–è§£è¯»é“¾æ¥ï¼š{text[:50]}... æ—¥æœŸï¼š{date_match.group(0)}")
                            policy_items.append(parent)
                            if len(policy_items) >= 5:
                                break
        
        # å°è¯•æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦æœ‰ä¸list-1-ajax-idç›¸å…³çš„AJAXè¯·æ±‚
        json_url = None
        if not policy_items:
            print("ğŸ” å°è¯•æ£€æŸ¥é¡µé¢ä¸­æ˜¯å¦æœ‰ä¸list-1-ajax-idç›¸å…³çš„AJAXè¯·æ±‚...")
            # æŸ¥æ‰¾æ‰€æœ‰scriptæ ‡ç­¾
            scripts = soup.find_all('script')
            for i, script in enumerate(scripts):
                script_content = script.string
                if script_content:
                    # æŸ¥æ‰¾ä¸list-1-ajax-idç›¸å…³çš„ä»£ç 
                    if 'list-1-ajax-id' in script_content:
                        print(f"ğŸ“ å‘ç°ä¸list-1-ajax-idç›¸å…³çš„scriptæ ‡ç­¾ #{i}ï¼Œé•¿åº¦ï¼š{len(script_content)}...")
                        print(f"ğŸ“ scriptå†…å®¹é¢„è§ˆï¼š{script_content[:2000]}...")
                        # å°è¯•æå–JSONæ–‡ä»¶URL
                        import re
                        json_match = re.search(r'url:\s*["\']([^"\']+)ZCJD_QZ\.json["\']', script_content)
                        if json_match:
                            json_path = json_match.group(1) + "ZCJD_QZ.json"
                            print(f"ğŸ“ æå–åˆ°JSONæ–‡ä»¶è·¯å¾„ï¼š{json_path}")
                            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                            if json_path.startswith('./'):
                                json_url = f"https://www.gov.cn/zhengce/jiedu/{json_path[2:]}"
                            else:
                                json_url = f"https://www.gov.cn/zhengce/jiedu/{json_path}"
                            print(f"âœ… æ„å»ºç»å¯¹JSONæ–‡ä»¶URLï¼š{json_url}")
                        break
        
        # å°è¯•è®¿é—®æ‰¾åˆ°çš„JSONæ•°æ®æ–‡ä»¶
        json_policies = []
        try:
            print("ğŸ” å°è¯•è®¿é—®æ‰¾åˆ°çš„JSONæ•°æ®æ–‡ä»¶...")
            # ä½¿ç”¨æå–çš„URLæˆ–é»˜è®¤URL
            if not json_url:
                json_url = "https://www.gov.cn/zhengce/jiedu/ZCJD_QZ.json"
            
            print(f"ğŸ“ å°è¯•è®¿é—®JSONæ–‡ä»¶ï¼š{json_url}")
            response = requests.get(json_url, timeout=15)
            if response.status_code == 200:
                print(f"âœ… JSONè¯·æ±‚æˆåŠŸï¼š{json_url}")
                print(f"ğŸ“ JSONå“åº”å†…å®¹é¢„è§ˆï¼š{response.text[:500]}...")
                # å°è¯•è§£æJSON
                try:
                    import json
                    data = response.json()
                    print("âœ… æˆåŠŸè§£æJSONæ•°æ®")
                    
                    # æ£€æŸ¥æ•°æ®ç»“æ„
                    if isinstance(data, list):
                        print(f"âœ… å‘ç°æ–‡ç« åˆ—è¡¨æ•°æ®ï¼ŒåŒ…å« {len(data)} ä¸ªæ–‡ç« ")
                        # éå†æ–‡ç« æ•°æ®
                        for article in data:
                            # æ£€æŸ¥æ–‡ç« æ•°æ®ç»“æ„
                            if isinstance(article, dict) and 'TITLE' in article and 'URL' in article and 'DOCRELPUBTIME' in article:
                                # æ„å»ºæ”¿ç­–æ•°æ®
                                pub_at = None
                                try:
                                    pub_at = datetime.strptime(article['DOCRELPUBTIME'], '%Y-%m-%d').date()
                                except Exception as e:
                                    print(f"âš ï¸  è§£ææ—¥æœŸå¤±è´¥ï¼š{e}")
                                
                                if pub_at:
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯å‰ä¸€å¤©çš„æ–‡ç« 
                                    if pub_at == yesterday:
                                        policy_data = {
                                            'title': article['TITLE'],
                                            'url': article['URL'] if article['URL'].startswith('http') else f"https://www.gov.cn{article['URL']}",
                                            'pub_at': pub_at,
                                            'content': '',  # å¯ä»¥åç»­å®ç°è¯¦æƒ…é¡µæŠ“å–
                                            'selected': False,
                                            'category': '',
                                            'source': 'ä¸­å›½æ”¿åºœç½‘'
                                        }
                                        json_policies.append(policy_data)
                                        print(f"âœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæ–‡ç« ï¼š{article['TITLE'][:50]}... æ—¥æœŸï¼š{article['DOCRELPUBTIME']}")
                        
                        print(f"âœ… æ€»å…±æ‰¾åˆ° {len(json_policies)} æ¡ç›®æ ‡æ—¥æœŸçš„æ–‡ç« ")
                        
                        if json_policies:
                            print(f"âœ… ä½¿ç”¨JSONæ•°æ®æ„å»ºæ”¿ç­–åˆ—è¡¨ï¼ŒåŒ…å« {len(json_policies)} æ¡æ•°æ®")
                            print(f"âœ… ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(json_policies)} æ¡å‰ä¸€å¤©æ•°æ®")
                            return json_policies
                    else:
                        print(f"ğŸ“ æ•°æ®ç»“æ„ï¼š{type(data)}")
                        if isinstance(data, dict):
                            print(f"ğŸ“ å­—å…¸é”®ï¼š{list(data.keys())}")
                except Exception as e:
                    print(f"âš ï¸  è§£æJSONå¤±è´¥ï¼š{e}")
            else:
                print(f"âš ï¸  JSONè¯·æ±‚å¤±è´¥ï¼š{json_url}ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
        except Exception as e:
            print(f"âš ï¸  è®¿é—®JSONæ–‡ä»¶å¤±è´¥ï¼š{e}")
        
        # æœ€åï¼Œå°è¯•æ‰“å°æ•´ä¸ªé¡µé¢çš„å‰3000ä¸ªå­—ç¬¦ï¼Œè¯¦ç»†äº†è§£é¡µé¢ç»“æ„
        if not policy_items:
            print("âš ï¸  ä»ç„¶æ²¡æœ‰æ‰¾åˆ°æ–‡ç« é¡¹ï¼Œæ‰“å°é¡µé¢å‰3000ä¸ªå­—ç¬¦è¯¦ç»†äº†è§£ç»“æ„...")
            page_preview = soup.prettify()[:3000]
            print(f"ğŸ“ é¡µé¢é¢„è§ˆï¼š{page_preview}...")
        
        print(f"\nğŸ“‹ æœ€ç»ˆæ‰¾åˆ° {len(policy_items)} ä¸ªæ–‡ç« é¡¹")
        
        filtered_count = 0
        
        for item in policy_items:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = item.select_one('a')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            policy_url = title_elem.get('href')
            
            # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
            if policy_url and not policy_url.startswith('http'):
                policy_url = f"https://www.gov.cn{policy_url}"
            
            # æå–å‘å¸ƒæ—¥æœŸ
            pub_at = None
            
            # å°è¯•ä¸åŒçš„æ—¥æœŸå…ƒç´ é€‰æ‹©å™¨
            # é¦–å…ˆå°è¯•ç”¨æˆ·æä¾›çš„xpathå¯¹åº”çš„ç»“æ„ï¼šh4/span
            date_selectors = [
                'h4 > span',         # h4ä¸‹çš„spanå…ƒç´ ï¼ˆç”¨æˆ·æä¾›çš„xpathç»“æ„ï¼‰
                '.date',            # classä¸ºdateçš„å…ƒç´ 
                'span.date',        # spanæ ‡ç­¾ä¸”classä¸ºdate
                '.time',            # classä¸ºtimeçš„å…ƒç´ 
                'span.time',        # spanæ ‡ç­¾ä¸”classä¸ºtime
                'span'              # æ‰€æœ‰spanå…ƒç´ 
            ]
            
            for selector in date_selectors:
                date_elem = item.select_one(selector)
                if date_elem:
                    date_str = date_elem.get_text(strip=True)
                    try:
                        # æ¸…ç†æ—¥æœŸå­—ç¬¦ä¸²ï¼ˆç§»é™¤å¤šä½™å­—ç¬¦ï¼‰
                        import re
                        date_match = re.search(r'\d{4}-\d{2}-\d{2}', date_str)
                        if date_match:
                            date_str = date_match.group(0)
                            pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                            break
                    except ValueError:
                        pass
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ—¥æœŸå…ƒç´ ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
            if not pub_at:
                text = item.get_text(strip=True)
                import re
                date_match = re.search(r'\d{4}-\d{2}-\d{2}', text)
                if date_match:
                    try:
                        date_str = date_match.group(0)
                        pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                    except ValueError:
                        pass
            
            # è°ƒè¯•ï¼šæ˜¾ç¤ºæå–çš„æ—¥æœŸ
            if pub_at:
                print(f"ğŸ“… æå–æ—¥æœŸï¼š{pub_at}ï¼Œç›®æ ‡æ—¥æœŸï¼š{yesterday}")
            else:
                print(f"â“ æœªæå–åˆ°æ—¥æœŸ - æ ‡é¢˜ï¼š{title[:30]}...")
            
            # è¿‡æ»¤ï¼šåªä¿ç•™å‰ä¸€å¤©çš„æ–‡ç« 
            if pub_at != yesterday:
                filtered_count += 1
                if pub_at:
                    print(f"â­ï¸  è¿‡æ»¤æ‰éç›®æ ‡æ—¥æœŸæ–‡ç« ï¼š{pub_at}")
                else:
                    print(f"â­ï¸  è¿‡æ»¤æ‰æ— æ—¥æœŸæ–‡ç« ")
                continue
            
            # è°ƒè¯•ï¼šæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ–‡ç« 
            print(f"âœ… æ‰¾åˆ°ç›®æ ‡æ—¥æœŸæ–‡ç« ï¼š{title[:30]}...")
            
            # æå–å†…å®¹ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦è¿›å…¥è¯¦æƒ…é¡µæŠ“å–ï¼‰
            content = ""  # å¯ä»¥åç»­å®ç°è¯¦æƒ…é¡µæŠ“å–
            
            # æ„å»ºæ”¿ç­–æ•°æ®
            policy_data = {
                'title': title,
                'url': policy_url,
                'pub_at': pub_at,
                'content': content,
                'selected': False,
                'category': '',  # ç•™ç©ºï¼Œä¸è®¾ç½®é»˜è®¤å€¼
                'source': 'ä¸­å›½æ”¿åºœç½‘'
            }
            
            policies.append(policy_data)
        
        print(f"âœ… ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
    
    return policies

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    
    ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“å·¥å…·å‡½æ•°
    """
    return save_to_policy(data_list, "ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«"""
    try:
        data = scrape_data()
        result = save_to_supabase(data)
        return result
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        return []

if __name__ == "__main__":
    run()
