import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
from supabase import create_client, Client

# ==========================================
# 1. åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
# ==========================================
SUPABASE_URL = os.environ.get("SUPABASE_PROJECT_API")
SUPABASE_KEY = os.environ.get("SUPABASE_ANON_PUBLIC")

def get_supabase_client() -> Client:
    if not SUPABASE_URL or not SUPABASE_KEY:
        raise ValueError("ç¼ºå°‘ Supabase ç¯å¢ƒå˜é‡: SUPABASE_PROJECT_API æˆ– SUPABASE_ANON_PUBLIC")
    return create_client(SUPABASE_URL, SUPABASE_KEY)

# ==========================================
# 2. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    """
    policies = []
    url = "https://www.gov.cn/zhengce/jiedu/index.htm"
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸ
        today = datetime.now().date()
        yesterday = today - timedelta(days=1)
        print(f"ğŸ“… è¿è¡Œæ—¥æœŸï¼š{today}")
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ”¿ç­–è§£è¯»åˆ—è¡¨ï¼ˆæ ¹æ®å®é™…ç½‘é¡µç»“æ„è°ƒæ•´é€‰æ‹©å™¨ï¼‰
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç½‘é¡µç»“æ„è¿›è¡Œè°ƒæ•´
        policy_items = soup.select('.list > li')
        
        filtered_count = 0
        
        for item in policy_items:
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title_elem = item.select_one('a')
            if not title_elem:
                continue
            
            title = title_elem.get_text(strip=True)
            policy_url = title_elem.get('href')
            
            # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
            if policy_url and not policy_url.startswith('http'):
                policy_url = f"https://www.gov.cn{policy_url}"
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_elem = item.select_one('.date')
            pub_at = None
            if date_elem:
                date_str = date_elem.get_text(strip=True)
                try:
                    pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                except ValueError:
                    pass
            
            # è¿‡æ»¤ï¼šåªä¿ç•™å‰ä¸€å¤©çš„æ–‡ç« 
            if pub_at != yesterday:
                filtered_count += 1
                continue
            
            # æå–å†…å®¹ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦è¿›å…¥è¯¦æƒ…é¡µæŠ“å–ï¼‰
            content = ""  # å¯ä»¥åç»­å®ç°è¯¦æƒ…é¡µæŠ“å–
            
            # æ„å»ºæ”¿ç­–æ•°æ®
            policy_data = {
                'title': title,
                'url': policy_url,
                'pub_at': pub_at,
                'content': content,
                'selected': False,
                'category': 'æ”¿ç­–è§£è¯»',
                'source': 'ä¸­å›½æ”¿åºœç½‘'
            }
            
            policies.append(policy_data)
        
        print(f"âœ… ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
    
    return policies

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    if not data_list:
        print("âš ï¸ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæ²¡æœ‰æŠ“å–åˆ°ä»»ä½•æ•°æ®ï¼Œè·³è¿‡å†™å…¥ã€‚")
        return []

    try:
        # è½¬æ¢dateå¯¹è±¡ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å…JSONåºåˆ—åŒ–é”™è¯¯
        processed_data = []
        for item in data_list:
            processed_item = item.copy()
            # æ£€æŸ¥pub_atæ˜¯å¦ä¸ºæ—¥æœŸå¯¹è±¡
            if hasattr(processed_item.get('pub_at'), 'isoformat'):
                processed_item['pub_at'] = processed_item['pub_at'].isoformat()
            processed_data.append(processed_item)
        
        supabase = get_supabase_client()
        response = supabase.table("policy").upsert(
            processed_data, 
            on_conflict="title"
        ).execute()
        
        print(f"âœ… ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæˆåŠŸå†™å…¥ {len(processed_data)} æ¡æ•°æ®åˆ° Supabase")
        return data_list  # è¿”å›åŸå§‹æ•°æ®ï¼Œä¿æŒä¸€è‡´æ€§
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šæ•°æ®åº“å†™å…¥å¤±è´¥ - {e}")
        return data_list  # å³ä½¿å†™å…¥å¤±è´¥ï¼Œä¹Ÿè¿”å›æŠ“å–çš„æ•°æ®ï¼Œç¡®ä¿ç»Ÿè®¡æ­£ç¡®

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«"""
    try:
        data = scrape_data()
        result = save_to_supabase(data)
        return result
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        return []

if __name__ == "__main__":
    run()
