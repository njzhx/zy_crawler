import time
import sys
from datetime import datetime
from io import StringIO

# å¯¼å…¥é£ä¹¦é€šçŸ¥æ¨¡å—
try:
    from feishu_notifier import send_crawler_result
except ImportError:
    send_crawler_result = None


class DualOutput:
    """åŒè¾“å‡ºæµï¼ŒåŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œç¼“å†²åŒº"""
    
    def __init__(self, original_stdout):
        self.original_stdout = original_stdout
        self.buffer = StringIO()
    
    def write(self, text):
        self.original_stdout.write(text)
        self.buffer.write(text)
    
    def flush(self):
        self.original_stdout.flush()
        self.buffer.flush()
    
    def getvalue(self):
        return self.buffer.getvalue()


# ==========================================
# çˆ¬è™«ç®¡ç†ç³»ç»Ÿ
# åŠŸèƒ½ï¼šæ‰§è¡Œå¤šä¸ªçˆ¬è™«ï¼Œä¸€ä¸ªçˆ¬è™«å‡ºé”™ä¸å½±å“å…¶ä»–çˆ¬è™«
# ==========================================

class CrawlerManager:
    def __init__(self):
        """åˆå§‹åŒ–çˆ¬è™«ç®¡ç†å™¨"""
        self.crawlers = []
        self.results = {}
    
    def register_crawler(self, name, crawler_func, crawler_module):
        """æ³¨å†Œçˆ¬è™«
        
        Args:
            name: çˆ¬è™«åç§°
            crawler_func: çˆ¬è™«æ‰§è¡Œå‡½æ•°
            crawler_module: çˆ¬è™«æ¨¡å—å¯¹è±¡ï¼Œç”¨äºè·å– TARGET_URL
        """
        target_url = getattr(crawler_module, 'TARGET_URL', '')
        self.crawlers.append((name, crawler_func, target_url))
        if target_url:
            print(f"âœ… å·²æ³¨å†Œçˆ¬è™«: {name} ({target_url})")
        else:
            print(f"âœ… å·²æ³¨å†Œçˆ¬è™«: {name}")
    
    def run_all_crawlers(self):
        """æ‰§è¡Œæ‰€æœ‰çˆ¬è™«
        
        Returns:
            dict: å„çˆ¬è™«æ‰§è¡Œç»“æœ
        """
        # å¼€å§‹æ•è·è¾“å‡º
        original_stdout = sys.stdout
        original_stderr = sys.stderr
        dual_out = DualOutput(original_stdout)
        dual_err = DualOutput(original_stderr)
        sys.stdout = dual_out
        sys.stderr = dual_err
        
        start_datetime = datetime.now()
        print(f"\nğŸš€ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ - {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)
        
        total_start_time = time.time()
        
        for name, crawler_func, target_url in self.crawlers:
            if target_url:
                print(f"\nğŸ“¦ å¼€å§‹æ‰§è¡Œçˆ¬è™«: {name}")
                print(f"ğŸ”— ç›®æ ‡ç½‘å€: {target_url}")
                print(f"   (HTML: <a href='{target_url}' target='_blank'>{name}</a>)")
            else:
                print(f"\nğŸ“¦ å¼€å§‹æ‰§è¡Œçˆ¬è™«: {name}")
            print("-" * 40)
            
            start_time = time.time()
            
            try:
                # æ‰§è¡Œçˆ¬è™«
                result = crawler_func()
                
                # è®°å½•ç»“æœ
                execution_time = time.time() - start_time
                
                # åŒºåˆ†æŠ“å–æ•°é‡å’Œå†™å…¥æ•°é‡
                # å‡è®¾ result åŒ…å«å®é™…å†™å…¥çš„æ•°æ®ï¼ˆå³ä½¿å†™å…¥å¤±è´¥ä¹Ÿè¿”å›æŠ“å–çš„æ•°æ®ï¼‰
                crawl_count = len(result)
                
                self.results[name] = {
                    'status': 'success',
                    'crawl_count': crawl_count,
                    'write_count': crawl_count,  # æš‚æ—¶ä½¿ç”¨ç›¸åŒå€¼ï¼Œåç»­å¯ä»çˆ¬è™«è¿”å›å€¼ä¸­è·å–
                    'execution_time': round(execution_time, 2),
                    'timestamp': datetime.now().isoformat()
                }
                
                print(f"âœ… çˆ¬è™« {name} æ‰§è¡ŒæˆåŠŸ")
                print(f"ğŸ“Š æŠ“å–æ•°æ®: {crawl_count} æ¡")
                print(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: {crawl_count} æ¡")
                print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {round(execution_time, 2)} ç§’")
                
            except Exception as e:
                # æ•è·å¼‚å¸¸ï¼Œç¡®ä¿å…¶ä»–çˆ¬è™«ç»§ç»­æ‰§è¡Œ
                execution_time = time.time() - start_time
                self.results[name] = {
                    'status': 'error',
                    'crawl_count': 0,
                    'write_count': 0,
                    'error_message': str(e),
                    'execution_time': round(execution_time, 2),
                    'timestamp': datetime.now().isoformat()
                }
                
                print(f"âŒ çˆ¬è™« {name} æ‰§è¡Œå¤±è´¥")
                print(f"ğŸ’¥ é”™è¯¯ä¿¡æ¯: {str(e)}")
                print(f"ğŸ“Š æŠ“å–æ•°æ®: 0 æ¡")
                print(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: 0 æ¡")
                print(f"â±ï¸  æ‰§è¡Œæ—¶é—´: {round(execution_time, 2)} ç§’")
            
            print("-" * 40)
        
        total_execution_time = time.time() - total_start_time
        end_datetime = datetime.now()
        
        print("=" * 60)
        print(f"ğŸ“‹ çˆ¬è™«æ‰§è¡Œå®Œæˆ - {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"â±ï¸  æ€»æ‰§è¡Œæ—¶é—´: {round(total_execution_time, 2)} ç§’")
        print(f"ğŸ“¦ æ‰§è¡Œçˆ¬è™«æ•°: {len(self.crawlers)}")
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in self.results.values() if r['status'] == 'success')
        error_count = sum(1 for r in self.results.values() if r['status'] == 'error')
        
        # ç»Ÿè®¡æ€»æŠ“å–å’Œå†™å…¥æ•°é‡
        total_crawl = sum(r.get('crawl_count', 0) for r in self.results.values())
        total_write = sum(r.get('write_count', 0) for r in self.results.values())
        
        print(f"âœ… æˆåŠŸ: {success_count} ä¸ª")
        print(f"âŒ å¤±è´¥: {error_count} ä¸ª")
        print(f"ğŸ“Š æ€»æŠ“å–æ•°æ®: {total_crawl} æ¡")
        print(f"ğŸ’¾ æ€»å†™å…¥æ•°æ®åº“: {total_write} æ¡")
        
        # è·å–å®Œæ•´æ—¥å¿—
        full_log = dual_out.getvalue() + dual_err.getvalue()
        
        # æ¢å¤æ ‡å‡†è¾“å‡º
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        
        # å‘é€é£ä¹¦é€šçŸ¥
        if send_crawler_result:
            print("\nğŸ“¤ æ­£åœ¨å‘é€é£ä¹¦é€šçŸ¥...")
            send_crawler_result(self.results, start_datetime, end_datetime, full_log)
        
        return self.results
    
    def get_summary(self):
        """è·å–æ‰§è¡Œæ‘˜è¦"""
        if not self.results:
            return "å°šæœªæ‰§è¡Œçˆ¬è™«ä»»åŠ¡"
        
        summary = []
        for name, result in self.results.items():
            if result['status'] == 'success':
                summary.append(f"âœ… {name}: æŠ“å– {result['crawl_count']} æ¡ï¼Œå†™å…¥æ•°æ®åº“ {result['write_count']} æ¡")
            else:
                summary.append(f"âŒ {name}: æ‰§è¡Œå¤±è´¥ - {result['error_message'][:100]}...")
        
        return "\n".join(summary)

# ==========================================
# ä¸»æ‰§è¡Œé€»è¾‘
# ==========================================
if __name__ == "__main__":
    # åˆ›å»ºçˆ¬è™«ç®¡ç†å™¨
    manager = CrawlerManager()
    
    # æ³¨å†Œçˆ¬è™«
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦æ ¹æ®å®é™…çˆ¬è™«æ¨¡å—è¿›è¡Œå¯¼å…¥å’Œæ³¨å†Œ
    try:
        # å¯¼å…¥ä¸­å›½æ”¿åºœç½‘çˆ¬è™«
        import gov_crawler
        manager.register_crawler("ä¸­å›½æ”¿åºœç½‘", gov_crawler.run, gov_crawler)
        
        # å¯¼å…¥ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»çˆ¬è™«
        import gov_interpretation_crawler
        manager.register_crawler("ä¸­å›½æ”¿åºœç½‘æ”¿ç­–è§£è¯»", gov_interpretation_crawler.run, gov_interpretation_crawler)
        
        # å¯¼å…¥å›½å®¶å‘æ”¹å§”çˆ¬è™«
        import ndrc_crawler
        manager.register_crawler("å›½å®¶å‘æ”¹å§”", ndrc_crawler.run, ndrc_crawler)
        
        # å¯¼å…¥äººæ°‘ç½‘è´¢ç»çˆ¬è™«
        import people_finance_crawler
        manager.register_crawler("äººæ°‘ç½‘è´¢ç»", people_finance_crawler.run, people_finance_crawler)
        
        # å°è¯•å¯¼å…¥æµ‹è¯•çˆ¬è™«ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼Œéå¿…éœ€ï¼‰
        try:
            import test_crawler
            manager.register_crawler("æµ‹è¯•çˆ¬è™«", test_crawler.run, test_crawler)
        except ImportError:
            print("âš ï¸  æµ‹è¯•çˆ¬è™«æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡æ³¨å†Œ")
        
        # åç»­æ·»åŠ å…¶ä»–çˆ¬è™«æ—¶ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ ¼å¼æ³¨å†Œ
        # import other_crawler
        # manager.register_crawler("å…¶ä»–ç½‘ç«™", other_crawler.run)
        
    except ImportError as e:
        print(f"âš ï¸  å¯¼å…¥çˆ¬è™«æ¨¡å—å¤±è´¥: {e}")
    
    # æ‰§è¡Œæ‰€æœ‰çˆ¬è™«
    if manager.crawlers:
        results = manager.run_all_crawlers()
        
        # æ‰“å°æ‰§è¡Œæ‘˜è¦
        print("\nğŸ“Š æ‰§è¡Œæ‘˜è¦:")
        print("=" * 60)
        print(manager.get_summary())
    else:
        print("âš ï¸  æ²¡æœ‰æ³¨å†Œä»»ä½•çˆ¬è™«")
