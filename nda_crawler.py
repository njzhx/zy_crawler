import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# çˆ¬è™«é…ç½®
TARGET_URL = "https://www.nda.gov.cn/sjj/zwgk/list/index_pc_1.html"


def scrape_data_test():
    """æµ‹è¯•ç‰ˆæœ¬ï¼šæŠ“å–æ•°æ®ï¼Œä¸è¿‡æ»¤æ—¥æœŸï¼Œè¿”å›æ‰€æœ‰æ•°æ®
    
    Returns:
        list: æŠ“å–åˆ°çš„æ•°æ®åˆ—è¡¨
    """
    policies = []
    url = TARGET_URL
    
    try:
        print(f"ğŸ” å¼€å§‹æµ‹è¯•çˆ¬è™«: æ•°æ®å±€_æ”¿åŠ¡å…¬å¼€")
        print(f"ğŸ”— ç›®æ ‡ç½‘å€: {url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« åˆ—è¡¨é¡¹
        items = soup.find_all('li')
        
        print(f"ğŸ“¦ æ‰¾åˆ° {len(items)} ä¸ªå¯èƒ½çš„åˆ—è¡¨é¡¹")
        
        for item in items:
            try:
                # æŸ¥æ‰¾é“¾æ¥
                a_tag = item.find('a')
                if not a_tag:
                    continue
                
                title = a_tag.get('title', '').strip() or a_tag.get_text(strip=True)
                href = a_tag.get('href', '')
                
                if not title or not href:
                    continue
                
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                if href.startswith('/'):
                    article_url = f"https://www.nda.gov.cn{href}"
                elif not href.startswith('http'):
                    article_url = f"https://www.nda.gov.cn/sjj/zwgk/list/{href}"
                else:
                    article_url = href
                
                # æå–å‘å¸ƒæ—¥æœŸ
                pub_at = None
                date_span = item.find('span')
                if date_span:
                    date_str = date_span.get_text(strip=True)
                    if date_str:
                        try:
                            # å°è¯•è§£æ YYYY.MM.DD æ ¼å¼
                            pub_at = datetime.strptime(date_str, '%Y.%m.%d').date()
                        except ValueError:
                            pass
                
                print(f"ğŸ“„ æ ‡é¢˜: {title}")
                print(f"   URL: {article_url}")
                print(f"   æ—¥æœŸ: {pub_at}")
                
                # æå–å†…å®¹ - æŠ“å–è¯¦æƒ…é¡µå†…å®¹
                content = ""
                try:
                    detail_response = requests.get(article_url, headers=headers, timeout=15)
                    detail_response.raise_for_status()
                    detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                    # å°è¯•æŸ¥æ‰¾å†…å®¹åŒºåŸŸ
                    content_elem = detail_soup.select_one('.content') or detail_soup.select_one('#content') or detail_soup.select_one('.zwgk-content')
                    if content_elem:
                        content = content_elem.get_text(strip=True)
                        print(f"   å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                    else:
                        print(f"   âš ï¸ æœªæ‰¾åˆ°å†…å®¹å…ƒç´ ")
                except Exception as e:
                    print(f"   âš ï¸ æŠ“å–è¯¦æƒ…é¡µå¤±è´¥: {e}")
                
                # æ„å»ºæ”¿ç­–æ•°æ®
                policy_data = {
                    'title': title,
                    'url': article_url,
                    'pub_at': pub_at,
                    'content': content,
                    'selected': False,
                    'category': '',
                    'source': 'å›½å®¶æ•°æ®å±€æ”¿åŠ¡å…¬å¼€'
                }
                
                policies.append(policy_data)
                print("-" * 60)
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†å•æ¡æ•°æ®å¤±è´¥: {e}")
                continue
        
        print(f"âœ… æˆåŠŸæŠ“å– {len(policies)} æ¡æ•°æ®")
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«å¤±è´¥ - {e}")
    
    return policies


def scrape_data():
    """æ­£å¼ç‰ˆæœ¬ï¼šæŠ“å–æ•°æ®ï¼ŒåªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    
    Returns:
        tuple: (policies, all_items)
            - policies: ç¬¦åˆç›®æ ‡æ—¥æœŸçš„æ•°æ®åˆ—è¡¨
            - all_items: æ‰€æœ‰æŠ“å–åˆ°çš„é¡¹ç›®ï¼ˆç”¨äºæ˜¾ç¤ºæœ€æ–°5æ¡ï¼‰
    """
    policies = []
    url = TARGET_URL
    all_items = []
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        

        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« åˆ—è¡¨é¡¹
        items = soup.find_all('li')
        
        filtered_count = 0
        
        for item in items:
            try:
                a_tag = item.find('a')
                if not a_tag:
                    continue
                
                title = a_tag.get('title', '').strip() or a_tag.get_text(strip=True)
                href = a_tag.get('href', '')
                
                if not title or not href:
                    continue
                
                # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                if href.startswith('/'):
                    article_url = f"https://www.nda.gov.cn{href}"
                else:
                    article_url = href
                
                # æå–å‘å¸ƒæ—¥æœŸ
                pub_at = None
                date_span = item.find('span')
                if date_span:
                    date_str = date_span.get_text(strip=True)
                    if date_str:
                        try:
                            pub_at = datetime.strptime(date_str, '%Y.%m.%d').date()
                        except ValueError:
                            pass
                
                # ä¿å­˜åˆ° all_items ç”¨äºæ˜¾ç¤ºæœ€æ–°5æ¡
                all_items.append({'title': title, 'pub_at': pub_at})
                
                # è¿‡æ»¤ï¼šåªä¿ç•™å‰ä¸€å¤©çš„æ–‡ç« 
                if pub_at != yesterday:
                    filtered_count += 1
                    continue
                
                # æå–å†…å®¹
                content = ""
                try:
                    detail_response = requests.get(article_url, headers=headers, timeout=15)
                    detail_response.raise_for_status()
                    detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                    content_elem = detail_soup.select_one('.content') or detail_soup.select_one('#content') or detail_soup.select_one('.zwgk-content')
                    if content_elem:
                        content = content_elem.get_text(strip=True)
                except Exception:
                    pass
                
                policy_data = {
                    'title': title,
                    'url': article_url,
                    'pub_at': pub_at,
                    'content': content,
                    'selected': False,
                    'category': '',
                    'source': 'å›½å®¶æ•°æ®å±€æ”¿åŠ¡å…¬å¼€'
                }
                
                policies.append(policy_data)
                
            except Exception:
                continue
        
        print(f"âœ… å›½å®¶æ•°æ®å±€çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
        # æ˜¾ç¤ºé¡µé¢æœ€æ–°5æ¡
        if all_items:
                print("ğŸ“Š é¡µé¢æœ€æ–°5æ¡æ˜¯ï¼š")
                for i, item in enumerate(all_items[:5], 1):
                    date_str = item['pub_at'].strftime('%Y-%m-%d') if item['pub_at'] else 'æœªçŸ¥æ—¥æœŸ'
                    print(f"âœ… {item['title']} {date_str}")
        
    except Exception as e:
        print(f"âŒ å›½å®¶æ•°æ®å±€çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
        print("----------------------------------------")
    
    return policies, all_items


def save_to_supabase(data_list):
    return save_to_policy(data_list, "å›½å®¶æ•°æ®å±€")


def run():
    """è¿è¡Œçˆ¬è™«"""
    try:
        data, _ = scrape_data()
        result = save_to_supabase(data)
        print(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: {len(data)} æ¡")
        print("----------------------------------------")
        return result
    except Exception as e:
        print(f"âŒ å›½å®¶æ•°æ®å±€çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        print("----------------------------------------")
        return []


def run_test():
    """æµ‹è¯•çˆ¬è™«"""
    print("=" * 60)
    print("ğŸ§ª å¼€å§‹æµ‹è¯•çˆ¬è™«")
    print("=" * 60)
    return scrape_data_test()


if __name__ == "__main__":
    # é»˜è®¤è¿è¡Œæµ‹è¯•ç‰ˆæœ¬
    run_test()

