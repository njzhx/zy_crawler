import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# çˆ¬è™«é…ç½®
TARGET_URL = "https://www.gov.cn/zhengce/zuixin/"

# ==========================================
# 2. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–ä¸­å›½æ”¿åºœç½‘æœ€æ–°æ”¿ç­–æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    """
    policies = []
    url = TARGET_URL
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        # åˆ›å»º UTC+8 æ—¶åŒº
        tz_utc8 = timezone(timedelta(hours=8))
        # è·å–åŒ—äº¬æ—¶é—´
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"ğŸ“… è¿è¡Œæ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ï¼š{today}")
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        # åŒæ—¶æ˜¾ç¤º UTC æ—¶é—´ï¼Œä¾¿äºè°ƒè¯•
        utc_now = datetime.utcnow()
        print(f"ğŸŒ è¿è¡Œæ—¶é—´ï¼ˆUTCï¼‰ï¼š{utc_now.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # å‘é€è¯·æ±‚
        response = requests.get(url, timeout=30)
        response.raise_for_status()
        
        # è§£æHTML
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾AJAXæ•°æ®URL
        ajax_url = None
        scripts = soup.find_all('script')
        for script in scripts:
            script_content = script.string
            if script_content and 'list-1-ajax-id' in script_content:
                import re
                # å°è¯•å¤šç§æ¨¡å¼åŒ¹é…AJAX URL
                patterns = [
                    r'url:\s*["\']([^"\']+)\.json["\']',
                    r'url:\s*["\']([^"\']+)\.json["\']',
                    r'ajax\s*:\s*["\']([^"\']+)["\']'
                ]
                
                for pattern in patterns:
                    ajax_match = re.search(pattern, script_content)
                    if ajax_match:
                        ajax_path = ajax_match.group(1)
                        if not ajax_path.endswith('.json'):
                            ajax_path += '.json'
                        
                        if ajax_path.startswith('http'):
                            ajax_url = ajax_path
                        elif ajax_path.startswith('./'):
                            ajax_url = f"https://www.gov.cn/zhengce/zuixin/{ajax_path[2:]}"
                        else:
                            ajax_url = f"https://www.gov.cn/zhengce/zuixin/{ajax_path}"
                        break
                if ajax_url:
                    break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°AJAX URLï¼Œå°è¯•å¸¸è§çš„JSONæ–‡ä»¶å
        if not ajax_url:
            common_json_names = [
                "https://www.gov.cn/zhengce/zuixin/data.json",
                "https://www.gov.cn/zhengce/zuixin/list.json",
                "https://www.gov.cn/zhengce/zuixin/zuixin.json"
            ]
            # å°è¯•ç¬¬ä¸€ä¸ªå¸¸è§URL
            ajax_url = common_json_names[0]
        
        # è¯·æ±‚AJAXæ•°æ®
        policy_items = []
        try:
            ajax_response = requests.get(ajax_url, timeout=15)
            if ajax_response.status_code == 200:
                import json
                data = ajax_response.json()
                
                if isinstance(data, list):
                    policy_items = data
        except Exception as e:
            print(f"âš ï¸  AJAXè¯·æ±‚å¼‚å¸¸: {e}")
        
        filtered_count = 0
        
        for item in policy_items:
            if not isinstance(item, dict):
                continue
            
            # æå–æ ‡é¢˜å’Œé“¾æ¥
            title = item.get('TITLE', '')
            policy_url = item.get('URL', '')
            
            if not title or not policy_url:
                continue
            
            # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
            if not policy_url.startswith('http'):
                policy_url = f"https://www.gov.cn{policy_url}"
            
            # æå–å‘å¸ƒæ—¥æœŸ
            date_str = item.get('DOCRELPUBTIME', '')
            pub_at = None
            if date_str:
                try:
                    pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                except ValueError:
                    pass
            

            
            # è¿‡æ»¤ï¼šåªä¿ç•™å‰ä¸€å¤©çš„æ–‡ç« 
            if pub_at != yesterday:
                filtered_count += 1
                continue
            
            # æå–å†…å®¹ - æŠ“å–è¯¦æƒ…é¡µå†…å®¹
            content = ""
            try:
                detail_response = requests.get(policy_url, timeout=15)
                detail_response.raise_for_status()
                detail_soup = BeautifulSoup(detail_response.content, 'html.parser')
                content_elem = detail_soup.select_one('#UCAP-CONTENT')
                if content_elem:
                    content = content_elem.get_text(strip=True)
            except Exception:
                pass
            
            # æ„å»ºæ”¿ç­–æ•°æ®
            policy_data = {
                'title': title,
                'url': policy_url,
                'pub_at': pub_at,
                'content': content,
                'selected': False,
                'category': 'æ”¿ç­–',
                'source': 'ä¸­å›½æ”¿åºœç½‘'
            }
            
            policies.append(policy_data)
        
        print(f"âœ… ä¸­å›½æ”¿åºœç½‘çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
    
    return policies

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    
    ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“å·¥å…·å‡½æ•°
    """
    return save_to_policy(data_list, "ä¸­å›½æ”¿åºœç½‘çˆ¬è™«")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œä¸­å›½æ”¿åºœç½‘çˆ¬è™«"""
    try:
        data = scrape_data()
        result = save_to_supabase(data)
        return result
    except Exception as e:
        print(f"âŒ ä¸­å›½æ”¿åºœç½‘çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        return []

if __name__ == "__main__":
    run()
