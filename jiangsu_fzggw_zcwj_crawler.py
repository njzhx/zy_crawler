
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Referer': 'https://fzggw.jiangsu.gov.cn/module/jslib/zcjd/zcjd.htm',
    'Content-Type': 'application/x-www-form-urlencoded',
    'Accept': 'application/json, text/javascript, */*; q=0.01',
    'X-Requested-With': 'XMLHttpRequest'
}

TARGET_URL = "https://fzggw.jiangsu.gov.cn/module/jslib/zcjd/zcjd.htm"


def scrape_data():
    policies = []
    all_items = []
    
    try:
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        

        
        # ‰ΩøÁî® API Êé•Âè£Ëé∑ÂèñÊï∞ÊçÆ
        api_url = "https://fzggw.jiangsu.gov.cn/module/jslib/zcjd/right.jsp"
        page_no = 1
        page_size = 10
        
        filtered_count = 0
        
        while True:
            data = {
                "name": "",
                "keytype": "",
                "year": "2026",
                "ztflid": "",
                "fwlbbm": "",
                "pageSize": page_size,
                "pageNo": page_no
            }
            
            response = requests.post(api_url, headers=headers, data=data, timeout=30)
            response.raise_for_status()
            
            # Ëß£Êûê JSON ÂìçÂ∫î
            import json
            json_data = json.loads(response.text)
            
            if not json_data.get('result'):
                break
            
            items = json_data.get('data', [])
            if not items:
                break
            
            for item in items:
                try:
                    title = item.get('vc_title', '').strip()
                    url = item.get('url', '')
                    c_deploytime = item.get('c_deploytime', '')
                    
                    if not title or len(title) < 5:
                        continue
                    
                    # ÊûÑÂª∫ÂÆåÊï¥ÁöÑÊñáÁ´† URL
                    if url.startswith('/'):
                        article_url = "https://fzggw.jiangsu.gov.cn" + url
                    elif not url.startswith('http'):
                        article_url = "https://fzggw.jiangsu.gov.cn/module/jslib/zcjd/" + url
                    else:
                        article_url = url
                    
                    # Ëß£ÊûêÂèëÂ∏ÉÊó∂Èó¥
                    pub_at = None
                    if c_deploytime:
                        try:
                            # ÂÅáËÆæÊó•ÊúüÊ†ºÂºè‰∏∫ YYYY-MM-DD
                            pub_at = datetime.strptime(c_deploytime, '%Y-%m-%d').date()
                        except ValueError:
                            pass
                    
                    # ‰øùÂ≠òÂà∞ all_items Áî®‰∫éÊòæÁ§∫ÊúÄÊñ∞5Êù°
                    all_items.append({'title': title, 'pub_at': pub_at})
                    
                    if pub_at != yesterday:
                        filtered_count += 1
                        continue
                    
                    # Ëé∑ÂèñÊñáÁ´†ÂÜÖÂÆπ
                    content = ""
                    try:
                        detail_resp = requests.get(article_url, headers=headers, timeout=15)
                        detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')
                        content_elem = detail_soup.select_one('.content') or detail_soup.select_one('#content')
                        if content_elem:
                            content = content_elem.get_text(strip=True)
                    except Exception:
                        pass
                    
                    policy_data = {
                        'title': title,
                        'url': article_url,
                        'pub_at': pub_at,
                        'content': content,
                        'selected': False,
                        'category': '',
                        'source': 'Ê±üËãèÁúÅÂèëÊîπÂßî'
                    }
                    policies.append(policy_data)
                    
                except Exception:
                    continue
            
            # Ê£ÄÊü•ÊòØÂê¶ËøòÊúâÊõ¥Â§öÈ°µÈù¢
            if len(items) < page_size:
                break
            page_no += 1
        
        print(f"‚úÖ Ê±üËãèÁúÅÂèëÊîπÂßîÁà¨Ëô´ÔºöÊàêÂäüÊäìÂèñ {len(policies)} Êù°Ââç‰∏ÄÂ§©Êï∞ÊçÆ")
        print(f"‚è≠Ô∏è  ËøáÊª§Êéâ {filtered_count} Êù°ÈùûÁõÆÊ†áÊó•ÊúüÁöÑÊï∞ÊçÆ")
        
        # ÊòæÁ§∫È°µÈù¢ÊúÄÊñ∞5Êù°
        if all_items:
            print("üìä È°µÈù¢ÊúÄÊñ∞5Êù°ÊòØÔºö")
            for i, item in enumerate(all_items[:5], 1):
                date_str = item['pub_at'].strftime('%Y-%m-%d') if item['pub_at'] else 'Êú™Áü•Êó•Êúü'
                print(f"‚úÖ {item['title']} {date_str}")
        
    except Exception as e:
        print(f"‚ùå Ê±üËãèÁúÅÂèëÊîπÂßîÁà¨Ëô´ÔºöÊäìÂèñÂ§±Ë¥• - {e}")
        print("----------------------------------------")
    
    return policies, all_items


def save_to_supabase(data_list):
    try:
        from db_utils import save_to_policy
        return save_to_policy(data_list, "Ê±üËãèÁúÅÂèëÊîπÂßî_ÊîøÁ≠ñÊñá‰ª∂")
    except Exception:
        return data_list


def run():
    try:
        data, _ = scrape_data()
        result = save_to_supabase(data)
        print(f"üíæ ÂÜôÂÖ•Êï∞ÊçÆÂ∫ì: {len(data)} Êù°")
        print("----------------------------------------")
        return result
    except Exception as e:
        print(f"‚ùå Ê±üËãèÁúÅÂèëÊîπÂßîÁà¨Ëô´ÔºöËøêË°åÂ§±Ë¥• - {e}")
        print("----------------------------------------")
        return []


if __name__ == "__main__":
    run()

