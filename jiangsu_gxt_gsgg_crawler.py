
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import re

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

TARGET_URL = "https://gxt.jiangsu.gov.cn/col/col6281/index.html"


def scrape_data():
    policies = []
    all_items = []
    url = TARGET_URL
    
    try:
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        

        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æŸ¥æ‰¾åŒ…å« datastore çš„ script æ ‡ç­¾
        script_tags = soup.find_all('script')
        datastore_script = None
        
        for script in script_tags:
            if script.string and '<datastore>' in script.string:
                datastore_script = script.string
                break
        
        filtered_count = 0
        
        if datastore_script:
            # æå– datastore å†…å®¹
            import re
            datastore_match = re.search(r'<datastore>([\s\S]*?)</datastore>', datastore_script)
            if datastore_match:
                datastore_content = datastore_match.group(1)
                
                # æå– recordset å†…å®¹
                recordset_match = re.search(r'<recordset>([\s\S]*?)</recordset>', datastore_content)
                if recordset_match:
                    recordset_content = recordset_match.group(1)
                    
                    # æå–æ‰€æœ‰ record å†…å®¹
                    records = re.findall(r'<record><!\[CDATA\[([\s\S]*?)\]\]></record>', recordset_content)
                    
                    for record in records:
                        try:
                            # è§£æ record ä¸­çš„ HTML
                            record_soup = BeautifulSoup(record, 'html.parser')
                            li = record_soup.find('li')
                            if not li:
                                continue
                            
                            a_tag = li.find('a')
                            if not a_tag:
                                continue
                            
                            title = a_tag.get('title', '').strip() or a_tag.get_text(strip=True)
                            href = a_tag.get('href', '')
                            
                            if not title or len(title) < 5:
                                continue
                            
                            if href.startswith('/'):
                                article_url = "https://gxt.jiangsu.gov.cn" + href
                            elif not href.startswith('http'):
                                article_url = "https://gxt.jiangsu.gov.cn/col/col6281/" + href
                            else:
                                article_url = href
                            
                            pub_at = None
                            date_text = li.get_text()
                            date_match = re.search(r'(\d{4})[-/\.](\d{1,2})[-/\.](\d{1,2})', date_text)
                            if date_match:
                                try:
                                    pub_at = datetime.strptime(f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", '%Y-%m-%d').date()
                                except ValueError:
                                    pass
                            
                            # ä¿å­˜åˆ° all_items ç”¨äºæ˜¾ç¤ºæœ€æ–°5æ¡
                            all_items.append({'title': title, 'pub_at': pub_at})
                            
                            if pub_at != yesterday:
                                filtered_count += 1
                                continue
                            
                            content = ""
                            try:
                                detail_resp = requests.get(article_url, headers=headers, timeout=15)
                                detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')
                                # ä¼˜å…ˆä½¿ç”¨ #con1ï¼Œç„¶åå°è¯•å…¶ä»–é€‰æ‹©å™¨
                                content_elem = detail_soup.select_one('#con1') or detail_soup.select_one('.content') or detail_soup.select_one('#content')
                                if content_elem:
                                    content = content_elem.get_text(strip=True)
                            except Exception:
                                pass
                            
                            policy_data = {
                                'title': title,
                                'url': article_url,
                                'pub_at': pub_at,
                                'content': content,
                                'selected': False,
                                'category': '',
                                'source': 'æ±Ÿè‹çœå·¥ä¿¡å…_å…¬ç¤ºå…¬å‘Š'
                            }
                            policies.append(policy_data)
                            print(f"  Found: {title}")
                            print(f"  URL: {article_url}")
                            print(f"  Date: {pub_at}")
                            
                        except Exception:
                            continue
        
        
        print(f"âœ… æ±Ÿè‹çœå·¥ä¿¡å…_å…¬ç¤ºå…¬å‘Šçˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
        # æ˜¾ç¤ºé¡µé¢æœ€æ–°5æ¡
        if all_items:
            print("ğŸ“Š é¡µé¢æœ€æ–°5æ¡æ˜¯ï¼š")
            for i, item in enumerate(all_items[:5], 1):
                date_str = item['pub_at'].strftime('%Y-%m-%d') if item['pub_at'] else 'æœªçŸ¥æ—¥æœŸ'
                print(f"âœ… {item['title']} {date_str}")
        
    except Exception as e:
        print(f"âŒ æ±Ÿè‹çœå·¥ä¿¡å…_å…¬ç¤ºå…¬å‘Šçˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
        print("----------------------------------------")
    
    return policies, all_items


def save_to_supabase(data_list):
    try:
        from db_utils import save_to_policy
        return save_to_policy(data_list, "æ±Ÿè‹çœå·¥ä¿¡å…_å…¬ç¤ºå…¬å‘Š")
    except Exception:
        return data_list


def run():
    try:
        data, _ = scrape_data()
        result = save_to_supabase(data)
        print(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: {len(data)} æ¡")
        print("----------------------------------------")
        return result
    except Exception as e:
        print(f"âŒ æ±Ÿè‹çœå·¥ä¿¡å…_å…¬ç¤ºå…¬å‘Šçˆ¬è™«ï¼šè¿è¡Œå¤±è´¥ - {e}")
        print("----------------------------------------")
        return []


if __name__ == "__main__":
    run()
