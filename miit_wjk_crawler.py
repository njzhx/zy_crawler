
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import re
import time

# Selenium ä¸ºå¯é€‰ä¾èµ–
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

TARGET_URL = "https://www.miit.gov.cn/search/zcwjk.html?websiteid=110000000000000&pg=&p=&tpl=14&category=183&q="


def scrape_data():
    policies = []
    url = TARGET_URL
    
    try:
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        # ä½¿ç”¨å‰ä¸€å¤©çš„æ—¥æœŸ
        yesterday = today - timedelta(days=1)
        # yesterday = datetime(2026, 2, 24).date()  # æµ‹è¯•ç”¨æˆ·æåˆ°çš„æ—¥æœŸ
        print(f"Date (Beijing): {today}")
        print(f"Target date: {yesterday}")
        
        print("Note: This site uses dynamic loading, trying different approaches...")
        
        # å°è¯•ç›´æ¥æ„é€ æœç´¢URLï¼ŒåŒ…å«æ—¥æœŸå‚æ•°
        search_url = f"https://www.miit.gov.cn/search/zcwjk.html?websiteid=110000000000000&pg=10&p=1&tpl=14&category=183&q=&begin={yesterday}&end={yesterday}"
        print(f"Trying search URL: {search_url}")
        
        # å°è¯•ç›´æ¥æœç´¢ç”¨æˆ·æåˆ°çš„å…·ä½“æ–‡ä»¶
        specific_url = f"https://www.miit.gov.cn/search/zcwjk.html?websiteid=110000000000000&pg=10&p=1&tpl=14&category=183&q=å·¥ä¸šå’Œä¿¡æ¯åŒ–éƒ¨åŠå…¬å…å…³äºå…¬å¸ƒæ•°å­—èµ‹èƒ½åŸºå±‚å‡è´Ÿå…¸å‹æ¡ˆä¾‹åå•çš„é€šçŸ¥"
        print(f"Trying specific file URL: {specific_url}")
        
        # å°è¯•ç›´æ¥è°ƒç”¨API
        print("\nTrying API approach...")
        api_url = "https://www.miit.gov.cn/search-front-server/api/search/info"
        
        # æ„å»ºæŸ¥è¯¢å‚æ•° - åŸºäºsearch.jsçš„åˆ†æ
        # å…ˆä½¿ç”¨ç®€å•æœç´¢è¯æµ‹è¯•APIæ˜¯å¦èƒ½è¿”å›ç»“æœ
        params = {
            "websiteid": "110000000000000",
            "scope": "basic",
            "q": "æ•°å­—èµ‹èƒ½åŸºå±‚å‡è´Ÿ",
            "pg": 10,
            "p": 1,
            "cateid": "183",
            "pos": "title_text,infocontent,titlepy",
            "_cus_eq_typename": "",  # å…¬æ–‡ç§ç±»
            "_cus_eq_publishgroupname": "",  # å‘å¸ƒæœºæ„
            "_cus_eq_themename": "",  # ä¸»é¢˜åˆ†ç±»
            # æš‚æ—¶ç§»é™¤æ—¥æœŸé™åˆ¶ï¼Œæµ‹è¯•APIæ˜¯å¦èƒ½è¿”å›ç»“æœ
            "dateField": "deploytime",
            "selectFields": "title,content,deploytime,_index,url,cdate,infoextends,infocontentattribute,columnname,filenumbername,publishgroupname,publishtime,metaid,bexxgk,columnid,xxgkextend1,xxgkextend2,themename,typename,indexcode,createdate",
            "group": "distinct",
            "highlightConfigs": "[{\"field\":\"infocontent\",\"numberOfFragments\":2,\"fragmentOffset\":0,\"fragmentSize\":30,\"noMatchSize\":145}]",
            "highlightFields": "title_text,infocontent,webid",
            "level": 6,
            "sortFields": "[{\"name\":\"deploytime\",\"type\":\"desc\"}]"
        }
        
        # ç§»é™¤Content-Typeå¤´ï¼Œä½¿ç”¨é»˜è®¤çš„GETè¯·æ±‚
        if 'Content-Type' in headers:
            del headers['Content-Type']
        response = requests.get(api_url, params=params, headers=headers, timeout=30)
        print(f"API Response status: {response.status_code}")
        
        if response.status_code == 200:
            try:
                data = response.json()
                print(f"API Response received: {type(data)}")
                # ä¿å­˜APIå“åº”ä»¥ä¾¿åˆ†æ
                with open('miit_api_response.json', 'w', encoding='utf-8') as f:
                    import json
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print("Saved API response to miit_api_response.json")
                
                # å¤„ç†APIå“åº”
                if data and 'data' in data and 'searchResult' in data['data']:
                    search_result = data['data']['searchResult']
                    print(f"Total hits: {search_result.get('totalHits', 0)}")
                    
                    if 'dataResults' in search_result and search_result['dataResults']:
                        data_results = search_result['dataResults']
                        print(f"Found {len(data_results)} items")
                        
                        for result in data_results:
                            try:
                                # å¤„ç†ç»“æœæ•°æ®
                                if 'groupData' in result and result['groupData']:
                                    group_data = result['groupData'][0]['data']
                                else:
                                    group_data = result['data']
                                
                                title = group_data.get('title', '') or group_data.get('title_text', '')
                                url = group_data.get('url', '')
                                deploytime = group_data.get('deploytime', '')
                                
                                if not title or not url:
                                    continue
                                
                                # æ„å»ºå®Œæ•´URL
                                if url.startswith('/'):
                                    article_url = "https://www.miit.gov.cn" + url
                                else:
                                    article_url = url
                                
                                # è§£ææ—¥æœŸ
                                pub_at = None
                                if deploytime:
                                    try:
                                        # å¤„ç†ä¸åŒæ ¼å¼çš„æ—¥æœŸ
                                        if isinstance(deploytime, str):
                                            if len(deploytime) == 10:
                                                pub_at = datetime.strptime(deploytime, '%Y-%m-%d').date()
                                            elif len(deploytime) == 19:
                                                pub_at = datetime.strptime(deploytime, '%Y-%m-%d %H:%M:%S').date()
                                    except ValueError:
                                        pass
                                
                                if pub_at != yesterday:
                                    continue
                                
                                # æŠ“å–å†…å®¹
                                content = ""
                                try:
                                    detail_resp = requests.get(article_url, headers=headers, timeout=15)
                                    detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')
                                    # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
                                    content_elem = detail_soup.select_one('.content') or detail_soup.select_one('#content') or detail_soup.select_one('.article-content') or detail_soup.select_one('.TRS_Editor')
                                    if content_elem:
                                        content = content_elem.get_text(strip=True)
                                except Exception as e:
                                    print(f"  Error fetching content: {e}")
                                    pass
                                
                                policy_data = {
                                    'title': title,
                                    'url': article_url,
                                    'pub_at': pub_at,
                                    'content': content,
                                    'selected': False,
                                    'category': 'æ–‡ä»¶åº“',
                                    'source': 'å·¥ä¿¡éƒ¨'
                                }
                                policies.append(policy_data)
                                print(f"  Found: {title}")
                                print(f"  URL: {article_url}")
                                print(f"  Date: {pub_at}")
                                print(f"  Content length: {len(content)} chars")
                                print("-" * 60)
                                
                            except Exception as e:
                                print(f"  Error processing API result: {e}")
                                continue
            except Exception as e:
                print(f"Error parsing JSON: {e}")
                # ä¿å­˜åŸå§‹å“åº”
                with open('miit_api_raw.txt', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print("Saved raw API response to miit_api_raw.txt")
        
        # å°è¯•ä¼ ç»Ÿæ–¹æ³• - æ—¥æœŸç­›é€‰
        response = requests.get(search_url, headers=headers, timeout=30)
        print(f"\nTraditional approach - Response status: {response.status_code}")
        
        if response.status_code == 200:
            # ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿åˆ†æ
            with open('miit_search_date.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("Saved date search page to miit_search_date.html")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"Page title: {soup.title.string}")
            
            # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
            search_content = soup.find('div', class_='search-conent')
            if search_content:
                print("Found search_content div")
                
                # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é¡¹
                items = search_content.find_all(['div', 'li'], class_=re.compile('result|item|article|list'))
                print(f"Found {len(items)} potential items")
        
        # å°è¯•ç›´æ¥æœç´¢å…·ä½“æ–‡ä»¶
        print(f"\nTrying specific file search...")
        response = requests.get(specific_url, headers=headers, timeout=30)
        print(f"Specific search response status: {response.status_code}")
        
        if response.status_code == 200:
            # ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿åˆ†æ
            with open('miit_search_specific.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("Saved specific search page to miit_search_specific.html")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"Page title: {soup.title.string}")
            
            # æŸ¥æ‰¾æœç´¢ç»“æœå®¹å™¨
            search_content = soup.find('div', class_='search-conent')
            if search_content:
                print("Found search_content div")
                
                # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ–‡ç« é¡¹
                items = search_content.find_all(['div', 'li'], class_=re.compile('result|item|article|list'))
                print(f"Found {len(items)} potential items")
                
                for item in items:
                    try:
                        # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
                        title_elem = item.find('h3') or item.find('a')
                        if title_elem:
                            if title_elem.name == 'h3':
                                a_tag = title_elem.find('a')
                            else:
                                a_tag = title_elem
                            
                            if a_tag:
                                title = a_tag.get('title', '').strip() or a_tag.get_text(strip=True)
                                href = a_tag.get('href', '')
                                
                                if not title or len(title) < 5:
                                    continue
                                
                                # æ„å»ºå®Œæ•´URL
                                if href.startswith('/'):
                                    article_url = "https://www.miit.gov.cn" + href
                                elif not href.startswith('http'):
                                    article_url = "https://www.miit.gov.cn/search/" + href
                                else:
                                    article_url = href
                                
                                # æŸ¥æ‰¾æ—¥æœŸ
                                pub_at = None
                                # å°è¯•ä»ä¸åŒä½ç½®æŸ¥æ‰¾æ—¥æœŸ
                                date_elems = item.find_all(['span', 'div'], class_=re.compile('date|time|å‘å¸ƒæ—¥æœŸ'))
                                for date_elem in date_elems:
                                    date_text = date_elem.get_text(strip=True)
                                    date_match = re.search(r'(\d{4})[-/\.](\d{1,2})[-/\.](\d{1,2})', date_text)
                                    if date_match:
                                        try:
                                            pub_at = datetime.strptime(f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", '%Y-%m-%d').date()
                                            break
                                        except ValueError:
                                            pass
                                
                                # å¦‚æœæ²¡æ‰¾åˆ°æ—¥æœŸï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
                                if not pub_at:
                                    date_match = re.search(r'(\d{4})[-/\.](\d{1,2})[-/\.](\d{1,2})', item.get_text())
                                    if date_match:
                                        try:
                                            pub_at = datetime.strptime(f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", '%Y-%m-%d').date()
                                        except ValueError:
                                            pass
                                
                                # æŠ“å–å†…å®¹
                                content = ""
                                try:
                                    detail_resp = requests.get(article_url, headers=headers, timeout=15)
                                    detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')
                                    # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
                                    content_elem = detail_soup.select_one('.content') or detail_soup.select_one('#content') or detail_soup.select_one('.article-content') or detail_soup.select_one('.TRS_Editor')
                                    if content_elem:
                                        content = content_elem.get_text(strip=True)
                                except Exception as e:
                                    print(f"  Error fetching content: {e}")
                                    pass
                                
                                policy_data = {
                                    'title': title,
                                    'url': article_url,
                                    'pub_at': pub_at,
                                    'content': content,
                                    'selected': False,
                                    'category': 'æ–‡ä»¶åº“',
                                    'source': 'å·¥ä¿¡éƒ¨'
                                }
                                policies.append(policy_data)
                                print(f"  Found: {title}")
                                print(f"  URL: {article_url}")
                                print(f"  Date: {pub_at}")
                                print(f"  Content length: {len(content)} chars")
                                print("-" * 60)
                                
                    except Exception as e:
                        print(f"  Error processing item: {e}")
                        continue
            else:
                print("No search content found")
        
        # å°è¯•å¦ä¸€ç§æ–¹æ³•ï¼šç›´æ¥è®¿é—®å¯èƒ½çš„åˆ—è¡¨é¡µ
        alternative_urls = [
            "https://www.miit.gov.cn/zwgk/zcwj/index.html",
            "https://www.miit.gov.cn/zwgk/zcwj/zfxxgk/index.html",
            "https://www.miit.gov.cn/search/xzgfxwjnew/index.html?websiteid=110000000000000&pg=&p=&tpl=14&category=51&q="  # è¡Œæ”¿è§„èŒƒæ€§æ–‡ä»¶å®é™…å†…å®¹
        ]
        
        for alt_url in alternative_urls:
            if policies:
                break
            
            print(f"\nTrying alternative URL: {alt_url}")
            try:
                response = requests.get(alt_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    # ä¿å­˜é¡µé¢å†…å®¹ä»¥ä¾¿åˆ†æ
                    if 'xzgfxwj' in alt_url:
                        with open('miit_xzgfxwj.html', 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        print("Saved xzgfxwj page to miit_xzgfxwj.html")
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    items = soup.find_all('li')
                    print(f"Found {len(items)} items")
                    
                    for item in items:
                        try:
                            a_tag = item.find('a')
                            if not a_tag:
                                continue
                            
                            title = a_tag.get('title', '').strip() or a_tag.get_text(strip=True)
                            href = a_tag.get('href', '')
                            
                            if not title or len(title) < 5:
                                continue
                            
                            if href.startswith('/'):
                                article_url = "https://www.miit.gov.cn" + href
                            else:
                                article_url = href
                            
                            # æŸ¥æ‰¾æ—¥æœŸ
                            pub_at = None
                            date_match = re.search(r'(\d{4})[-/\.](\d{1,2})[-/\.](\d{1,2})', item.get_text())
                            if date_match:
                                try:
                                    pub_at = datetime.strptime(f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", '%Y-%m-%d').date()
                                except ValueError:
                                    pass
                            
                            if pub_at != yesterday:
                                continue
                            
                            # æŠ“å–å†…å®¹
                            content = ""
                            try:
                                detail_resp = requests.get(article_url, headers=headers, timeout=15)
                                detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')
                                content_elem = detail_soup.select_one('.content') or detail_soup.select_one('#content')
                                if content_elem:
                                    content = content_elem.get_text(strip=True)
                            except Exception:
                                pass
                            
                            policy_data = {
                                'title': title,
                                'url': article_url,
                                'pub_at': pub_at,
                                'content': content,
                                'selected': False,
                                'category': 'æ–‡ä»¶åº“',
                                'source': 'å·¥ä¿¡éƒ¨'
                            }
                            policies.append(policy_data)
                            print(f"  Found: {title}")
                            print(f"  URL: {article_url}")
                            print(f"  Date: {pub_at}")
                            print(f"  Content length: {len(content)} chars")
                            print("-" * 60)
                            
                        except Exception as e:
                            print(f"  Error processing alternative item: {e}")
                            continue
            except Exception as e:
                print(f"Error with alternative URL {alt_url}: {e}")
                continue
        
        # å°è¯•ä½¿ç”¨Seleniumè·å–åŠ¨æ€å†…å®¹
        if not policies and SELENIUM_AVAILABLE:
            print("\nTrying Selenium approach...")
            try:
                # é…ç½®Chromeé€‰é¡¹
                chrome_options = Options()
                chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
                chrome_options.add_argument('--disable-gpu')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--window-size=1920,1080')
                chrome_options.add_argument(f'user-agent={headers["User-Agent"]}')
                
                # åˆå§‹åŒ–æµè§ˆå™¨ - ä½¿ç”¨webdriver-manager
                service = Service(ChromeDriverManager().install())
                driver = webdriver.Chrome(service=service, options=chrome_options)
                driver.set_page_load_timeout(60)
                
                # è®¿é—®æœç´¢é¡µé¢
                search_url = f"https://www.miit.gov.cn/search/zcwjk.html?websiteid=110000000000000&pg=10&p=1&tpl=14&category=183&q=æ•°å­—èµ‹èƒ½åŸºå±‚å‡è´Ÿ"
                print(f"Selenium visiting: {search_url}")
                driver.get(search_url)
                
                # ç­‰å¾…é¡µé¢åŠ è½½
                time.sleep(5)  # ç»™JavaScriptæ—¶é—´åŠ è½½å†…å®¹
                
                # ä¿å­˜é¡µé¢å†…å®¹
                page_source = driver.page_source
                with open('miit_selenium_page.html', 'w', encoding='utf-8') as f:
                    f.write(page_source)
                print("Saved Selenium page to miit_selenium_page.html")
                
                # è§£æé¡µé¢
                soup = BeautifulSoup(page_source, 'html.parser')
                
                # æŸ¥æ‰¾æœç´¢ç»“æœ
                search_content = soup.find('div', class_='search-con')
                if search_content:
                    print("Found search-con div with Selenium")
                    
                    # æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« é¡¹
                    items = search_content.find_all('div', class_='jcse-result-box')
                    print(f"Found {len(items)} items with Selenium")
                    
                    for item in items:
                        try:
                            # æŸ¥æ‰¾æ ‡é¢˜å’Œé“¾æ¥
                            title_elem = item.find('a')
                            if title_elem:
                                title = title_elem.get_text(strip=True)
                                href = title_elem.get('href', '')
                                
                                if not title or len(title) < 5:
                                    continue
                                
                                # æ„å»ºå®Œæ•´URL
                                if href.startswith('/'):
                                    article_url = "https://www.miit.gov.cn" + href
                                else:
                                    article_url = href
                                
                                # æŸ¥æ‰¾æ—¥æœŸ
                                pub_at = None
                                date_elem = item.find('span', text=re.compile(r'\d{4}-\d{2}-\d{2}'))
                                if date_elem:
                                    date_text = date_elem.get_text(strip=True)
                                    date_match = re.search(r'(\d{4})[-/\.](\d{1,2})[-/\.](\d{1,2})', date_text)
                                    if date_match:
                                        try:
                                            pub_at = datetime.strptime(f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", '%Y-%m-%d').date()
                                        except ValueError:
                                            pass
                                
                                # å¦‚æœæ²¡æ‰¾åˆ°æ—¥æœŸï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
                                if not pub_at:
                                    date_match = re.search(r'(\d{4})[-/\.](\d{1,2})[-/\.](\d{1,2})', item.get_text())
                                    if date_match:
                                        try:
                                            pub_at = datetime.strptime(f"{date_match.group(1)}-{date_match.group(2)}-{date_match.group(3)}", '%Y-%m-%d').date()
                                        except ValueError:
                                            pass
                                
                                if pub_at != yesterday:
                                    continue
                                
                                # æŠ“å–å†…å®¹
                                content = ""
                                try:
                                    driver.get(article_url)
                                    time.sleep(5)  # å¢åŠ ç­‰å¾…æ—¶é—´
                                    detail_soup = BeautifulSoup(driver.page_source, 'html.parser')
                                    # å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨
                                    content_selectors = [
                                        '.content',
                                        '#content',
                                        '.article-content',
                                        '.TRS_Editor',
                                        '.article-body',
                                        '.main-content',
                                        '.article-main'
                                    ]
                                    for selector in content_selectors:
                                        content_elem = detail_soup.select_one(selector)
                                        if content_elem:
                                            content = content_elem.get_text(strip=True)
                                            if content:
                                                break
                                    # å¦‚æœè¿˜æ˜¯æ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•è·å–æ‰€æœ‰pæ ‡ç­¾å†…å®¹
                                    if not content:
                                        paragraphs = detail_soup.find_all('p')
                                        if paragraphs:
                                            content = ' '.join([p.get_text(strip=True) for p in paragraphs])
                                    print(f"  Content fetched successfully: {len(content) > 0}")
                                except Exception as e:
                                    print(f"  Error fetching content with Selenium: {e}")
                                    pass
                                
                                policy_data = {
                                    'title': title,
                                    'url': article_url,
                                    'pub_at': pub_at,
                                    'content': content,
                                    'selected': False,
                                    'category': 'æ–‡ä»¶åº“',
                                    'source': 'å·¥ä¿¡éƒ¨'
                                }
                                policies.append(policy_data)
                                print(f"  Found with Selenium: {title}")
                                print(f"  URL: {article_url}")
                                print(f"  Date: {pub_at}")
                                print(f"  Content length: {len(content)} chars")
                                print("-" * 60)
                                
                        except Exception as e:
                            print(f"  Error processing Selenium item: {e}")
                            continue
                else:
                    print("No search content found with Selenium")
                
                # å…³é—­æµè§ˆå™¨
                driver.quit()
                
            except Exception as e:
                print(f"Selenium error: {e}")
                try:
                    driver.quit()
                except:
                    pass
        elif not policies and not SELENIUM_AVAILABLE:
            print("\nSelenium not available, skipping Selenium approach")
        
        print(f"Found {len(policies)} items for target date")
        
    except Exception as e:
        print(f"Error: {e}")
    
    return policies


def save_to_supabase(data_list):
    try:
        from db_utils import save_to_policy
        return save_to_policy(data_list, "å·¥ä¿¡éƒ¨_æ–‡ä»¶åº“")
    except Exception as e:
        print(f"Error saving to database: {e}")
        return data_list


def run():
    try:
        data = scrape_data()
        save_to_supabase(data)
        return data
    except Exception as e:
        print(f"Run failed: {e}")
        return []


def run_test():
    """æµ‹è¯•ç‰ˆæœ¬"""
    print("=" * 60)
    print("ğŸ§ª Testing MIIT File Library Crawler")
    print("=" * 60)
    
    try:
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"Date (Beijing): {today}")
        print(f"Target date: {yesterday}")
        
        # æµ‹è¯•ç›´æ¥æœç´¢URL
        search_url = f"https://www.miit.gov.cn/search/zcwjk.html?websiteid=110000000000000&pg=10&p=1&tpl=14&category=183&q=&begin={yesterday}&end={yesterday}"
        print(f"Testing search URL: {search_url}")
        
        response = requests.get(search_url, headers=headers, timeout=30)
        print(f"Response status: {response.status_code}")
        
        if response.status_code == 200:
            # ä¿å­˜æ•´ä¸ªé¡µé¢
            with open('miit_full_page.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("Saved full page to miit_full_page.html")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"Page title: {soup.title.string}")
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            search_content = soup.find('div', class_='search-conent')
            if search_content:
                print("Found search content div")
                # ä¿å­˜å†…å®¹ä»¥ä¾¿åˆ†æ
                with open('miit_search_result.html', 'w', encoding='utf-8') as f:
                    f.write(str(search_content))
                print("Saved search content to miit_search_result.html")
            else:
                print("No search content found")
        
    except Exception as e:
        print(f"Test failed: {e}")


if __name__ == "__main__":
    # é»˜è®¤è¿è¡Œæ­£å¼ç‰ˆæœ¬
    run()
    # å¦‚éœ€è¿è¡Œæµ‹è¯•ç‰ˆæœ¬ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # run_test()

