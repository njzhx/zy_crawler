
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone
import re
import time

# Selenium ä¸ºå¯é€‰ä¾èµ–
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1'
}

TARGET_URL = "https://www.miit.gov.cn/search/zcwjk.html?websiteid=110000000000000&pg=&p=&tpl=14&category=183&q="


def scrape_data():
    policies = []
    all_items = []
    url = TARGET_URL
    
    try:
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        # ä½¿ç”¨å‰ä¸€å¤©çš„æ—¥æœŸ
        yesterday = today - timedelta(days=1)
        # yesterday = datetime(2026, 2, 24).date()  # æµ‹è¯•æœ‰æ•°æ®çš„æ—¥æœŸ
        
        filtered_count = 0
        
        # 1. å…ˆè·å–åˆ†ç±»ä¿¡æ¯
        category_api_url = "https://www.miit.gov.cn/search-front-server/api/structure/list-category"
        category_params = {
            "websiteid": "110000000000000",
            "searchid": "183"  # ä»URLå‚æ•°è·å–çš„categoryå€¼
        }
        
        category_response = requests.get(category_api_url, params=category_params, headers=headers, timeout=30)
        
        cateid = "183"  # é»˜è®¤å€¼
        if category_response.status_code == 200:
            try:
                category_data = category_response.json()
                if category_data and 'data' in category_data and 'categories' in category_data['data']:
                    categories = category_data['data']['categories']
                    if categories:
                        cateid = categories[0].get('iid', '183')
            except Exception:
                pass
        
        # 2. ä½¿ç”¨æ­£ç¡®çš„åˆ†ç±»IDè¿›è¡Œæœç´¢
        api_url = "https://www.miit.gov.cn/search-front-server/api/search/info"
        
        # æ„å»ºæŸ¥è¯¢å‚æ•° - åŸºäºsearch.jsçš„åˆ†æ
        # ä¸è®¾ç½®æ—¥æœŸé™åˆ¶ï¼Œè·å–æ‰€æœ‰æ•°æ®ååœ¨æœ¬åœ°ç­›é€‰
        params = {
            "websiteid": "110000000000000",
            "scope": "basic",
            "q": "",  # ç©ºæœç´¢è¯ï¼Œè·å–æ‰€æœ‰æ•°æ®
            "pg": 50,  # å¢åŠ æ¯é¡µæ•°é‡
            "p": 1,
            "cateid": cateid,
            "pos": "title_text,infocontent,titlepy",
            "_cus_eq_typename": "",  # å…¬æ–‡ç§ç±»
            "_cus_eq_publishgroupname": "",  # å‘å¸ƒæœºæ„
            "_cus_eq_themename": "",  # ä¸»é¢˜åˆ†ç±»
            # ä¸è®¾ç½®æ—¥æœŸé™åˆ¶ï¼Œè·å–æ‰€æœ‰æ•°æ®
            "dateField": "deploytime",
            "selectFields": "title,content,deploytime,_index,url,cdate,infoextends,infocontentattribute,columnname,filenumbername,publishgroupname,publishtime,metaid,bexxgk,columnid,xxgkextend1,xxgkextend2,themename,typename,indexcode,createdate",
            "group": "distinct",
            "highlightConfigs": "[{\"field\":\"infocontent\",\"numberOfFragments\":2,\"fragmentOffset\":0,\"fragmentSize\":30,\"noMatchSize\":145}]",
            "highlightFields": "title_text,infocontent,webid",
            "level": 6,
            "sortFields": "[{\"name\":\"deploytime\",\"type\":\"desc\"}]"
        }
        
        # ç§»é™¤Content-Typeå¤´ï¼Œä½¿ç”¨é»˜è®¤çš„GETè¯·æ±‚
        if 'Content-Type' in headers:
            del headers['Content-Type']
        response = requests.get(api_url, params=params, headers=headers, timeout=30)
        
        if response.status_code == 200:
            try:
                data = response.json()
                
                # å¤„ç†APIå“åº”
                if data and 'data' in data and 'searchResult' in data['data']:
                    search_result = data['data']['searchResult']
                    
                    if 'dataResults' in search_result and search_result['dataResults']:
                        data_results = search_result['dataResults']
                        
                        for result in data_results:
                            try:
                                # å¤„ç†ç»“æœæ•°æ®
                                if 'groupData' in result and result['groupData']:
                                    group_data = result['groupData'][0]['data']
                                else:
                                    group_data = result['data']
                                
                                title = group_data.get('title', '') or group_data.get('title_text', '')
                                url = group_data.get('url', '')
                                deploytime = group_data.get('deploytime', '')
                                
                                if not title or not url:
                                    continue
                                
                                # æ„å»ºå®Œæ•´URL
                                if url.startswith('/'):
                                    article_url = "https://www.miit.gov.cn" + url
                                else:
                                    article_url = url
                                
                                # è§£ææ—¥æœŸ
                                pub_at = None
                                
                                # ä¼˜å…ˆä½¿ç”¨jsearch_dateå­—æ®µï¼ˆå·²ç»æ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼‰
                                if 'jsearch_date' in group_data:
                                    jsearch_date = group_data.get('jsearch_date', '')
                                    if jsearch_date:
                                        try:
                                            pub_at = datetime.strptime(jsearch_date, '%Y-%m-%d').date()
                                        except ValueError:
                                            pass
                                
                                # å¦‚æœæ²¡æœ‰jsearch_dateï¼Œå°è¯•è§£ææ—¶é—´æˆ³æ ¼å¼çš„æ—¥æœŸ
                                if not pub_at and deploytime:
                                    try:
                                        # å¤„ç†æ—¶é—´æˆ³æ ¼å¼
                                        if isinstance(deploytime, str):
                                            # å°è¯•å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºæ—¥æœŸ
                                            timestamp = int(deploytime) / 1000  # æ¯«ç§’è½¬ç§’
                                            pub_at = datetime.fromtimestamp(timestamp, tz=timezone(timedelta(hours=8))).date()
                                    except (ValueError, TypeError):
                                        pass
                                
                                # å°è¯•å…¶ä»–æ—¥æœŸå­—æ®µ
                                if not pub_at:
                                    # å°è¯•cdateå­—æ®µ
                                    cdate = group_data.get('cdate', '')
                                    if cdate:
                                        try:
                                            timestamp = int(cdate) / 1000
                                            pub_at = datetime.fromtimestamp(timestamp, tz=timezone(timedelta(hours=8))).date()
                                        except (ValueError, TypeError):
                                            pass
                                
                                # ä¿å­˜åˆ° all_items ç”¨äºæ˜¾ç¤ºæœ€æ–°5æ¡
                                all_items.append({'title': title, 'pub_at': pub_at})
                                
                                if pub_at != yesterday:
                                    filtered_count += 1
                                    continue
                                
                                # æŠ“å–å†…å®¹
                                content = ""
                                try:
                                    detail_resp = requests.get(article_url, headers=headers, timeout=15)
                                    detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')
                                    # ä¼˜å…ˆä½¿ç”¨ #con_conï¼Œç„¶åå°è¯•å…¶ä»–é€‰æ‹©å™¨
                                    content_elem = detail_soup.select_one('#con_con') or detail_soup.select_one('.content') or detail_soup.select_one('#content') or detail_soup.select_one('.article-content') or detail_soup.select_one('.TRS_Editor')
                                    if content_elem:
                                        content = content_elem.get_text(strip=True)
                                except Exception:
                                    pass
                                
                                policy_data = {
                                    'title': title,
                                    'url': article_url,
                                    'pub_at': pub_at,
                                    'content': content,
                                    'selected': False,
                                    'category': '',
                                    'source': 'å·¥ä¿¡éƒ¨'
                                }
                                policies.append(policy_data)
                                
                            except Exception:
                                continue
            except Exception:
                pass
        
        # æ˜¾ç¤ºç»“æœ
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        print(f"âœ… å·¥ä¿¡éƒ¨çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
        # æ˜¾ç¤ºé¡µé¢æœ€æ–°5æ¡
        if all_items:
            print("ğŸ“Š é¡µé¢æœ€æ–°5æ¡æ˜¯ï¼š")
            for i, item in enumerate(all_items[:5], 1):
                date_str = item['pub_at'].strftime('%Y-%m-%d') if item['pub_at'] else 'æœªçŸ¥æ—¥æœŸ'
                # æ§åˆ¶æ ‡é¢˜é•¿åº¦ä¸º10ä¸ªæ±‰å­—
                title = item['title']
                if len(title) > 10:
                    title = title[:10] + "..."
                print(f"âœ… {title} {date_str}")
        
    except Exception as e:
        print(f"âŒ å·¥ä¿¡éƒ¨çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
        print("----------------------------------------")
    
    return policies, all_items


def save_to_supabase(data_list):
    try:
        from db_utils import save_to_policy
        return save_to_policy(data_list, "å·¥ä¿¡éƒ¨_æ–‡ä»¶åº“")
    except Exception as e:
        print(f"Error saving to database: {e}")
        return data_list


def run():
    try:
        #print("ğŸ“¦ å¼€å§‹æ‰§è¡Œçˆ¬è™«: å·¥ä¿¡éƒ¨_æ–‡ä»¶åº“")
        #print(f"ğŸ”— ç›®æ ‡ç½‘å€: `{TARGET_URL}`")
        #print("----------------------------------------")
        data, _ = scrape_data()
        result = save_to_supabase(data)
        print(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: {len(data)} æ¡")
        print("----------------------------------------")
        return result
    except Exception as e:
        print(f"âŒ å·¥ä¿¡éƒ¨çˆ¬è™«ï¼šè¿è¡Œå¤±è´¥ - {e}")
        print("----------------------------------------")
        return []


def run_test():
    """æµ‹è¯•ç‰ˆæœ¬"""
    print("=" * 60)
    print("ğŸ§ª Testing MIIT File Library Crawler")
    print("=" * 60)
    
    try:
        tz_utc8 = timezone(timedelta(hours=8))
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"Date (Beijing): {today}")
        print(f"Target date: {yesterday}")
        
        # æµ‹è¯•ç›´æ¥æœç´¢URL
        search_url = f"https://www.miit.gov.cn/search/zcwjk.html?websiteid=110000000000000&pg=10&p=1&tpl=14&category=183&q=&begin={yesterday}&end={yesterday}"
        print(f"Testing search URL: {search_url}")
        
        response = requests.get(search_url, headers=headers, timeout=30)
        print(f"Response status: {response.status_code}")
        
        if response.status_code == 200:
            # ä¿å­˜æ•´ä¸ªé¡µé¢
            with open('miit_full_page.html', 'w', encoding='utf-8') as f:
                f.write(response.text)
            print("Saved full page to miit_full_page.html")
            
            soup = BeautifulSoup(response.content, 'html.parser')
            print(f"Page title: {soup.title.string}")
            
            # æŸ¥æ‰¾æœç´¢ç»“æœ
            search_content = soup.find('div', class_='search-conent')
            if search_content:
                print("Found search content div")
                # ä¿å­˜å†…å®¹ä»¥ä¾¿åˆ†æ
                with open('miit_search_result.html', 'w', encoding='utf-8') as f:
                    f.write(str(search_content))
                print("Saved search content to miit_search_result.html")
            else:
                print("No search content found")
        
    except Exception as e:
        print(f"Test failed: {e}")


if __name__ == "__main__":
    # é»˜è®¤è¿è¡Œæ­£å¼ç‰ˆæœ¬
    run()
    # å¦‚éœ€è¿è¡Œæµ‹è¯•ç‰ˆæœ¬ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # run_test()

