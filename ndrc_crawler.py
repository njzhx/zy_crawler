import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# çˆ¬è™«é…ç½®
TARGET_URL = "https://www.ndrc.gov.cn/xxgk/wjk/"

# ==========================================
# 2. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–å›½å®¶å‘æ”¹å§”æ–‡ä»¶åº“æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    """
    policies = []
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        # åˆ›å»º UTC+8 æ—¶åŒº
        tz_utc8 = timezone(timedelta(hours=8))
        # è·å–åŒ—äº¬æ—¶é—´
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        print(f"ğŸ“… è¿è¡Œæ—¥æœŸï¼ˆåŒ—äº¬æ—¶é—´ï¼‰ï¼š{today}")
        print(f"ğŸ¯ ç›®æ ‡æŠ“å–æ—¥æœŸï¼š{yesterday}")
        # åŒæ—¶æ˜¾ç¤º UTC æ—¶é—´ï¼Œä¾¿äºè°ƒè¯•
        utc_now = datetime.utcnow()
        print(f"ğŸŒ è¿è¡Œæ—¶é—´ï¼ˆUTCï¼‰ï¼š{utc_now.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # ç›´æ¥è°ƒç”¨APIæ¥å£
        print("\nğŸš€ ç›´æ¥è°ƒç”¨APIæ¥å£è·å–æ•°æ®...")
        api_url = "https://fwfx.ndrc.gov.cn/api/query"
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        params = {
            'qt': '',  # æœç´¢å…³é”®è¯
            'tab': 'all',  # æ‰€æœ‰æ–‡ä»¶ç±»å‹
            'page': 1,  # é¡µç 
            'pageSize': 20,  # æ¯é¡µæ•°é‡
            'siteCode': 'bm04000fgk',  # ç«™ç‚¹ä»£ç 
            'key': 'CAB549A94CF659904A7D6B0E8FC8A7E9',  # å¯†é’¥
            'startDateStr': yesterday.strftime('%Y-%m-%d'),  # å¼€å§‹æ—¥æœŸ
            'endDateStr': yesterday.strftime('%Y-%m-%d'),  # ç»“æŸæ—¥æœŸ
            'timeOption': 2,  # æ—¶é—´é€‰é¡¹ï¼š2è¡¨ç¤ºå…·ä½“æ—¥æœŸ
            'sort': 'dateDesc'  # æŒ‰æ—¥æœŸé™åºæ’åº
        }
        
        # å‘é€è¯·æ±‚
        response = requests.get(api_url, params=params, timeout=30)
        response.raise_for_status()
        
        # è§£æJSONå“åº”
        import json
        data = response.json()
        print(f"âœ… APIè¯·æ±‚æˆåŠŸï¼ŒçŠ¶æ€ï¼š{data.get('ok', False)}")
        
        # å¤„ç†å“åº”æ•°æ®
        if data.get('ok', False):
            result_list = data.get('data', {}).get('resultList', [])
            print(f"ğŸ“‹ æ‰¾åˆ° {len(result_list)} æ¡æ•°æ®")
            filtered_count = 0
            
            for item in result_list:
                # æå–æ•°æ®
                title = item.get('title', '')
                policy_url = item.get('url', '')
                doc_date = item.get('docDate', '')
                
                # è§£ææ—¥æœŸ
                pub_at = None
                if doc_date:
                    try:
                        pub_at = datetime.strptime(doc_date.split(' ')[0], '%Y-%m-%d').date()
                    except ValueError:
                        pass
                
                # è¿‡æ»¤ï¼šåªä¿ç•™ç›®æ ‡æ—¥æœŸçš„æ–‡ç« 
                if pub_at == yesterday:
                    # æå–å†…å®¹ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦è¿›å…¥è¯¦æƒ…é¡µæŠ“å–ï¼‰
                    content = ""  # å¯ä»¥åç»­å®ç°è¯¦æƒ…é¡µæŠ“å–
                    
                    # æ„å»ºæ”¿ç­–æ•°æ®
                    policy_data = {
                        'title': title,
                        'url': policy_url,
                        'pub_at': pub_at,
                        'content': content,
                        'selected': False,
                        'category': '',
                        'source': 'å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼šå‘æ”¹å§”æ–‡ä»¶'
                    }
                    
                    policies.append(policy_data)
                else:
                    filtered_count += 1
        else:
            print(f"âŒ APIè¯·æ±‚å¤±è´¥ï¼š{data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
        
        print(f"âœ… å›½å®¶å‘æ”¹å§”çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
    except Exception as e:
        print(f"âŒ å›½å®¶å‘æ”¹å§”çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
    
    return policies

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    
    ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“å·¥å…·å‡½æ•°
    """
    return save_to_policy(data_list, "å›½å®¶å‘æ”¹å§”çˆ¬è™«")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œå›½å®¶å‘æ”¹å§”çˆ¬è™«"""
    try:
        data = scrape_data()
        result = save_to_supabase(data)
        return result
    except Exception as e:
        print(f"âŒ å›½å®¶å‘æ”¹å§”çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        return []

if __name__ == "__main__":
    run()
