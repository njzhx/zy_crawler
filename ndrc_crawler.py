import os
import requests
import re
import time
from bs4 import BeautifulSoup
from datetime import datetime, timedelta

# å°è¯•å¯¼å…¥ Selenium
SELENIUM_AVAILABLE = False
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.service import Service
    SELENIUM_AVAILABLE = True
except ImportError:
    print("Selenium æœªå®‰è£…ï¼Œå°†å°è¯•å…¶ä»–æ–¹æ³•")

# å¯¼å…¥æ•°æ®åº“å·¥å…·
from db_utils import save_to_policy

# çˆ¬è™«é…ç½®
TARGET_URL = "https://www.ndrc.gov.cn/xxgk/wjk/"

# ==========================================
# 2. ç½‘é¡µæŠ“å–é€»è¾‘
# ==========================================
def scrape_data():
    """æŠ“å–å›½å®¶å‘æ”¹å§”æ–‡ä»¶åº“æ•°æ®
    
    åªæŠ“å–å‰ä¸€å¤©å‘å¸ƒçš„æ–‡ç« 
    ä¾‹å¦‚ï¼šè¿è¡Œæ—¶æ˜¯2026å¹´2æœˆ18æ—¥ï¼ŒåªæŠ“å–2026å¹´2æœˆ17æ—¥çš„æ–‡ç« 
    
    Returns:
        tuple: (policies, all_items)
            - policies: ç¬¦åˆç›®æ ‡æ—¥æœŸçš„æ•°æ®åˆ—è¡¨
            - all_items: æ‰€æœ‰æŠ“å–åˆ°çš„é¡¹ç›®ï¼ˆç”¨äºæ˜¾ç¤ºæœ€æ–°5æ¡ï¼‰
    """
    policies = []
    all_items = []
    
    try:
        # è®¡ç®—å‰ä¸€å¤©æ—¥æœŸï¼ˆä½¿ç”¨åŒ—äº¬æ—¶é—´ UTC+8ï¼‰
        from datetime import timezone
        # åˆ›å»º UTC+8 æ—¶åŒº
        tz_utc8 = timezone(timedelta(hours=8))
        # è·å–åŒ—äº¬æ—¶é—´
        today = datetime.now(tz_utc8).date()
        yesterday = today - timedelta(days=1)
        
        # ç›´æ¥è°ƒç”¨ API æ¥å£ï¼ˆä»é¡µé¢ä»£ç ä¸­æå–çš„æ­£ç¡® APIï¼‰
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        api_url = "https://fwfx.ndrc.gov.cn/api/query"
        
        # æ„å»ºæ­£ç¡®çš„è¯·æ±‚å‚æ•°ï¼ˆä»é¡µé¢ä»£ç ä¸­æå–ï¼‰
        params = {
            'qt': '',  # æœç´¢å…³é”®è¯
            'tab': 'all',  # æ‰€æœ‰æ–‡ä»¶ç±»å‹
            'page': 1,  # é¡µç 
            'pageSize': 50,  # æ¯é¡µæ•°é‡
            'siteCode': 'bm04000fgk',  # ç«™ç‚¹ä»£ç 
            'key': 'CAB549A94CF659904A7D6B0E8FC8A7E9',  # å¯†é’¥
            'startDateStr': '',  # å¼€å§‹æ—¥æœŸï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸é™åˆ¶ï¼‰
            'endDateStr': '',  # ç»“æŸæ—¥æœŸï¼ˆç©ºå­—ç¬¦ä¸²è¡¨ç¤ºä¸é™åˆ¶ï¼‰
            'timeOption': 0,  # æ—¶é—´é€‰é¡¹ï¼š0è¡¨ç¤ºä¸é™åˆ¶
            'sort': 'dateDesc'  # æŒ‰æ—¥æœŸé™åºæ’åº
        }
        
        response = requests.get(api_url, headers=headers, params=params, timeout=30)
        response.raise_for_status()
        
        # è§£æ JSON
        import json
        data = response.json()
        
        items = []
        if data.get('ok', False):
            result_list = data.get('data', {}).get('resultList', [])
            for item in result_list:
                if isinstance(item, dict):
                    title = item.get('title', '')
                    href = item.get('url', '')
                    doc_date = item.get('docDate', '')
                    
                    if title and href and doc_date:
                        # æå–æ—¥æœŸéƒ¨åˆ†
                        date_str = doc_date.split(' ')[0]
                        items.append((title, href, date_str))
        
        print(f"ğŸ“‹ æ‰¾åˆ° {len(items)} æ¡æ•°æ®")
        filtered_count = 0
        
        for title, href, date_str in items:
            # è§£ææ—¥æœŸ
            pub_at = None
            if date_str:
                try:
                    pub_at = datetime.strptime(date_str, '%Y-%m-%d').date()
                except ValueError:
                    pass
            
            # æ„å»ºå®Œæ•´URL
            policy_url = href
            if not policy_url.startswith('http'):
                if policy_url.startswith('/'):
                    policy_url = f"https://www.ndrc.gov.cn{policy_url}"
                else:
                    policy_url = f"https://www.ndrc.gov.cn/xxgk/wjk/{policy_url}"
            
            # ä¿å­˜åˆ° all_items ç”¨äºæ˜¾ç¤ºæœ€æ–°5æ¡
            all_items.append({'title': title, 'pub_at': pub_at})
            
            # è¿‡æ»¤ï¼šåªä¿ç•™ç›®æ ‡æ—¥æœŸçš„æ–‡ç« 
            if pub_at == yesterday:
                # æå–å†…å®¹ï¼ˆè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…å¯èƒ½éœ€è¦è¿›å…¥è¯¦æƒ…é¡µæŠ“å–ï¼‰
                content = ""  # å¯ä»¥åç»­å®ç°è¯¦æƒ…é¡µæŠ“å–
                
                # æ„å»ºæ”¿ç­–æ•°æ®
                policy_data = {
                    'title': title,
                    'url': policy_url,
                    'pub_at': pub_at,
                    'content': content,
                    'selected': False,
                    'category': '',
                    'source': 'å›½å®¶å‘å±•å’Œæ”¹é©å§”å‘˜ä¼šå‘æ”¹å§”æ–‡ä»¶'
                }
                
                policies.append(policy_data)
            else:
                filtered_count += 1
        
        print(f"âœ… å›½å®¶å‘æ”¹å§”çˆ¬è™«ï¼šæˆåŠŸæŠ“å– {len(policies)} æ¡å‰ä¸€å¤©æ•°æ®")
        print(f"â­ï¸  è¿‡æ»¤æ‰ {filtered_count} æ¡éç›®æ ‡æ—¥æœŸçš„æ•°æ®")
        
        # æ˜¾ç¤ºé¡µé¢æœ€æ–°5æ¡
        if all_items:
            print("ğŸ“Š é¡µé¢æœ€æ–°5æ¡æ˜¯ï¼š")
            for i, item in enumerate(all_items[:5], 1):
                date_str = item['pub_at'].strftime('%Y-%m-%d') if item['pub_at'] else 'æœªçŸ¥æ—¥æœŸ'
                print(f"âœ… {item['title']} {date_str}")
        
    except Exception as e:
        print(f"âŒ å›½å®¶å‘æ”¹å§”çˆ¬è™«ï¼šæŠ“å–å¤±è´¥ - {e}")
        print("----------------------------------------")
    
    return policies, all_items

# ==========================================
# 3. æ•°æ®å…¥åº“é€»è¾‘
# ==========================================
def save_to_supabase(data_list):
    """ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“
    
    ä½¿ç”¨ç»Ÿä¸€çš„æ•°æ®åº“å·¥å…·å‡½æ•°
    """
    return save_to_policy(data_list, "å›½å®¶å‘æ”¹å§”çˆ¬è™«")

# ==========================================
# ä¸»å‡½æ•°
# ==========================================
def run():
    """è¿è¡Œå›½å®¶å‘æ”¹å§”çˆ¬è™«"""
    try:
        data, _ = scrape_data()
        result = save_to_supabase(data)
        print(f"ğŸ’¾ å†™å…¥æ•°æ®åº“: {len(data)} æ¡")
        print("----------------------------------------")
        return result
    except Exception as e:
        print(f"âŒ å›½å®¶å‘æ”¹å§”çˆ¬è™«ï¼šè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªæ•è·çš„å¼‚å¸¸ - {e}")
        print("----------------------------------------")
        return []

if __name__ == "__main__":
    run()
